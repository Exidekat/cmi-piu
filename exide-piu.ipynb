{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "\n",
    "from catboost import CatBoostRegressor, MultiTargetCustomMetric\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from numpy.typing import ArrayLike, NDArray\n",
    "from polars.testing import assert_frame_equal\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"Failed to optimize method\")\n",
    "\n",
    "\n",
    "#DATA_DIR = Path(\"/kaggle/input/child-mind-institute-problematic-internet-use\")\n",
    "DATA_DIR = Path(\"./data\")\n",
    "\n",
    "TARGET_COLS = [\n",
    "    \"PCIAT-PCIAT_01\",\n",
    "    \"PCIAT-PCIAT_02\",\n",
    "    \"PCIAT-PCIAT_03\",\n",
    "    \"PCIAT-PCIAT_04\",\n",
    "    \"PCIAT-PCIAT_05\",\n",
    "    \"PCIAT-PCIAT_06\",\n",
    "    \"PCIAT-PCIAT_07\",\n",
    "    \"PCIAT-PCIAT_08\",\n",
    "    \"PCIAT-PCIAT_09\",\n",
    "    \"PCIAT-PCIAT_10\",\n",
    "    \"PCIAT-PCIAT_11\",\n",
    "    \"PCIAT-PCIAT_12\",\n",
    "    \"PCIAT-PCIAT_13\",\n",
    "    \"PCIAT-PCIAT_14\",\n",
    "    \"PCIAT-PCIAT_15\",\n",
    "    \"PCIAT-PCIAT_16\",\n",
    "    \"PCIAT-PCIAT_17\",\n",
    "    \"PCIAT-PCIAT_18\",\n",
    "    \"PCIAT-PCIAT_19\",\n",
    "    \"PCIAT-PCIAT_20\",\n",
    "    \"PCIAT-PCIAT_Total\",\n",
    "    \"sii\",\n",
    "]\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    \"Basic_Demos-Age\",\n",
    "    \"PreInt_EduHx-computerinternet_hoursday\",\n",
    "    \"SDS-SDS_Total_Raw\",\n",
    "    \"Physical-Height\",\n",
    "    \"FGC-FGC_TL\",\n",
    "    \"Physical-Waist_Circumference\",\n",
    "    \"BIA-BIA_FMI\",\n",
    "    \"BIA-BIA_Fat\",\n",
    "    \"Fitness_Endurance-Time_Sec\",\n",
    "    \"Fitness_Endurance-Max_Stage\",\n",
    "    \"FGC-FGC_CU_Zone\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pl.read_csv(DATA_DIR / \"train.csv\")\n",
    "test = pl.read_csv(DATA_DIR / \"test.csv\")\n",
    "train_test = pl.concat([train, test], how=\"diagonal\")\n",
    "\n",
    "IS_TEST = test.height <= 100\n",
    "\n",
    "assert_frame_equal(train, train_test[: train.height].select(train.columns))\n",
    "assert_frame_equal(test, train_test[train.height :].select(test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast string columns to categorical\n",
    "train_test = train_test.with_columns(cs.string().cast(pl.Categorical).fill_null(\"NAN\"))\n",
    "train = train_test[: train.height]\n",
    "test = train_test[train.height :]\n",
    "\n",
    "# ignore rows with null values in TARGET_COLS\n",
    "train_without_null = train_test.drop_nulls(subset=TARGET_COLS)\n",
    "X = train_without_null.select(FEATURE_COLS)\n",
    "X_test = test.select(FEATURE_COLS)\n",
    "y = train_without_null.select(TARGET_COLS)\n",
    "y_sii = y.get_column(\"sii\").to_numpy()  # ground truth\n",
    "cat_features = X.select(cs.categorical()).columns\n",
    "\n",
    "print(\"Features selected:\")\n",
    "print(cat_features)  # Should be none"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Tubotubo's Quadratic Weighted Kappa metric & Optuna optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTargetQWK(MultiTargetCustomMetric):\n",
    "    def get_final_error(self, error, weight):\n",
    "        return np.sum(error)  # / np.sum(weight)\n",
    "\n",
    "    def is_max_optimal(self):\n",
    "        # if True, the bigger the better\n",
    "        return True\n",
    "\n",
    "    def evaluate(self, approxes, targets, weight):\n",
    "        # approxes: 予測値 (shape: [ターゲット数, サンプル数])\n",
    "        # targets: 実際の値 (shape: [ターゲット数, サンプル数])\n",
    "        # weight: サンプルごとの重み (Noneも可)\n",
    "\n",
    "        approx = np.clip(approxes[-1], 0, 3).round().astype(int)\n",
    "        target = targets[-1]\n",
    "\n",
    "        qwk = cohen_kappa_score(target, approx, weights=\"quadratic\")\n",
    "\n",
    "        return qwk, 1\n",
    "\n",
    "    def get_custom_metric_name(self):\n",
    "        return \"MultiTargetQWK\"\n",
    "\n",
    "\n",
    "class OptimizedRounder:\n",
    "    \"\"\"\n",
    "    A class for optimizing the rounding of continuous predictions into discrete class labels using Optuna.\n",
    "    The optimization process maximizes the Quadratic Weighted Kappa score by learning thresholds that separate\n",
    "    continuous predictions into class intervals.\n",
    "\n",
    "    Args:\n",
    "        n_classes (int): The number of discrete class labels.\n",
    "        n_trials (int, optional): The number of trials for the Optuna optimization. Defaults to 100.\n",
    "\n",
    "    Attributes:\n",
    "        n_classes (int): The number of discrete class labels.\n",
    "        labels (NDArray[np.int_]): An array of class labels from 0 to `n_classes - 1`.\n",
    "        n_trials (int): The number of optimization trials.\n",
    "        metric (Callable): The Quadratic Weighted Kappa score metric used for optimization.\n",
    "        thresholds (List[float]): The optimized thresholds learned after calling `fit()`.\n",
    "\n",
    "    Methods:\n",
    "        fit(y_pred: NDArray[np.float_], y_true: NDArray[np.int_]) -> None:\n",
    "            Fits the rounding thresholds based on continuous predictions and ground truth labels.\n",
    "\n",
    "            Args:\n",
    "                y_pred (NDArray[np.float_]): Continuous predictions that need to be rounded.\n",
    "                y_true (NDArray[np.int_]): Ground truth class labels.\n",
    "\n",
    "            Returns:\n",
    "                None\n",
    "\n",
    "        predict(y_pred: NDArray[np.float_]) -> NDArray[np.int_]:\n",
    "            Predicts discrete class labels by rounding continuous predictions using the fitted thresholds.\n",
    "            `fit()` must be called before `predict()`.\n",
    "\n",
    "            Args:\n",
    "                y_pred (NDArray[np.float_]): Continuous predictions to be rounded.\n",
    "\n",
    "            Returns:\n",
    "                NDArray[np.int_]: Predicted class labels.\n",
    "\n",
    "        _normalize(y: NDArray[np.float_]) -> NDArray[np.float_]:\n",
    "            Normalizes the continuous values to the range [0, `n_classes - 1`].\n",
    "\n",
    "            Args:\n",
    "                y (NDArray[np.float_]): Continuous values to be normalized.\n",
    "\n",
    "            Returns:\n",
    "                NDArray[np.float_]: Normalized values.\n",
    "\n",
    "    References:\n",
    "        - This implementation uses Optuna for threshold optimization.\n",
    "        - Quadratic Weighted Kappa is used as the evaluation metric.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_classes: int, n_trials: int = 100):\n",
    "        self.n_classes = n_classes\n",
    "        self.labels = np.arange(n_classes)\n",
    "        self.n_trials = n_trials\n",
    "        self.metric = partial(cohen_kappa_score, weights=\"quadratic\")\n",
    "\n",
    "    def fit(self, y_pred: NDArray[np.float_], y_true: NDArray[np.int_]) -> None:\n",
    "        y_pred = self._normalize(y_pred)\n",
    "\n",
    "        def objective(trial: optuna.Trial) -> float:\n",
    "            thresholds = []\n",
    "            for i in range(self.n_classes - 1):\n",
    "                low = max(thresholds) if i > 0 else min(self.labels)\n",
    "                high = max(self.labels)\n",
    "                th = trial.suggest_float(f\"threshold_{i}\", low, high)\n",
    "                thresholds.append(th)\n",
    "            try:\n",
    "                y_pred_rounded = np.digitize(y_pred, thresholds)\n",
    "            except ValueError:\n",
    "                return -100\n",
    "            return self.metric(y_true, y_pred_rounded)\n",
    "\n",
    "        optuna.logging.disable_default_handler()\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(\n",
    "            objective,\n",
    "            n_trials=self.n_trials,\n",
    "        )\n",
    "        self.thresholds = [study.best_params[f\"threshold_{i}\"] for i in range(self.n_classes - 1)]\n",
    "\n",
    "    def predict(self, y_pred: NDArray[np.float_]) -> NDArray[np.int_]:\n",
    "        assert hasattr(self, \"thresholds\"), \"fit() must be called before predict()\"\n",
    "        y_pred = self._normalize(y_pred)\n",
    "        return np.digitize(y_pred, self.thresholds)\n",
    "\n",
    "    def _normalize(self, y: NDArray[np.float_]) -> NDArray[np.float_]:\n",
    "        # normalize y_pred to [0, n_classes - 1]\n",
    "        return (y - y.min()) / (y.max() - y.min()) * (self.n_classes - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Start making models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting catboost parameters\n",
    "params = dict(\n",
    "    loss_function=\"MultiRMSE\",\n",
    "    eval_metric=MultiTargetQWK(),\n",
    "    iterations=1 if IS_TEST else 100000,\n",
    "    learning_rate=0.1,\n",
    "    depth=5,\n",
    "    early_stopping_rounds=50,\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=52)\n",
    "models: list[CatBoostRegressor] = []\n",
    "y_pred = np.full((X.height, len(TARGET_COLS)), fill_value=np.nan)\n",
    "for train_idx, val_idx in skf.split(X, y_sii):\n",
    "    X_train: pl.DataFrame\n",
    "    X_val: pl.DataFrame\n",
    "    y_train: pl.DataFrame\n",
    "    y_val: pl.DataFrame\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # train model\n",
    "    model = CatBoostRegressor(**params)\n",
    "    model.fit(\n",
    "        X_train.to_pandas(),\n",
    "        y_train.to_pandas(),\n",
    "        eval_set=(X_val.to_pandas(), y_val.to_pandas()),\n",
    "        cat_features=cat_features,\n",
    "        verbose=False,\n",
    "    )\n",
    "    models.append(model)\n",
    "\n",
    "    # predict\n",
    "    y_pred[val_idx] = model.predict(X_val.to_pandas())\n",
    "\n",
    "assert np.isnan(y_pred).sum() == 0\n",
    "# Optimize thresholds\n",
    "optimizer = OptimizedRounder(n_classes=4, n_trials=300)\n",
    "y_pred_total = y_pred[:, TARGET_COLS.index(\"PCIAT-PCIAT_Total\")]\n",
    "optimizer.fit(y_pred_total, y_sii)\n",
    "y_pred_rounded = optimizer.predict(y_pred_total)\n",
    "\n",
    "# Calculate QWK\n",
    "qwk = cohen_kappa_score(y_sii, y_pred_rounded, weights=\"quadratic\")\n",
    "print(f\"Cross-Validated QWK Score: {qwk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare the CatBoost models\n",
    "for model in models:\n",
    "    #print(X, y_sii)\n",
    "    y_pred = model.predict(X.to_pandas())\n",
    "    #print(X.shape)\n",
    "    #print(y_sii.shape)\n",
    "    #print(y_pred.shape)\n",
    "    y_pred_total = y_pred[:, TARGET_COLS.index(\"PCIAT-PCIAT_Total\")]\n",
    "    #print(y_pred_total.shape)\n",
    "    \n",
    "    y_pred_rounded = optimizer.predict(y_pred_total)    \n",
    "    qwk = cohen_kappa_score(y_sii, y_pred_rounded, weights=\"quadratic\")\n",
    "    print(f\"Cross-Validated QWK Score: {qwk}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Trying out XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting xgboost parameters\n",
    "'''params = dict(\n",
    "    loss_function=\"MultiRMSE\",\n",
    "    eval_metric=MultiTargetQWK(),\n",
    "    iterations=1 if IS_TEST else 100000,\n",
    "    learning_rate=0.1,\n",
    "    depth=5,\n",
    "    early_stopping_rounds=50,\n",
    ")'''\n",
    "\n",
    "params = dict(\n",
    "    n_estimators=1000, \n",
    "    max_depth=7, \n",
    "    eta=0.1, \n",
    "    subsample=0.7, \n",
    "    colsample_bytree=0.8\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=52)\n",
    "models: list[XGBRegressor] = []\n",
    "y_pred = np.full((X.height, len(TARGET_COLS)), fill_value=np.nan)\n",
    "for train_idx, val_idx in skf.split(X, y_sii):\n",
    "    \n",
    "    X_train: pl.DataFrame\n",
    "    X_val: pl.DataFrame\n",
    "    y_train: pl.DataFrame\n",
    "    y_val: pl.DataFrame\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # train model\n",
    "    model = XGBRegressor(**params)\n",
    "    #print(X_train)\n",
    "    #print(y_train)\n",
    "    model.fit(\n",
    "        X_train.to_pandas(),\n",
    "        y_train.to_pandas(),\n",
    "        eval_set=[(X_val.to_pandas(), y_val.to_pandas())],\n",
    "        verbose=False,\n",
    "    )\n",
    "    models.append(model)\n",
    "\n",
    "    # predict\n",
    "    y_pred[val_idx] = model.predict(X_val.to_pandas())\n",
    "\n",
    "assert np.isnan(y_pred).sum() == 0\n",
    "# Optimize thresholds\n",
    "optimizer = OptimizedRounder(n_classes=4, n_trials=300)\n",
    "y_pred_total = y_pred[:, TARGET_COLS.index(\"PCIAT-PCIAT_Total\")]\n",
    "optimizer.fit(y_pred_total, y_sii)\n",
    "y_pred_rounded = optimizer.predict(y_pred_total)\n",
    "\n",
    "# Calculate QWK\n",
    "qwk = cohen_kappa_score(y_sii, y_pred_rounded, weights=\"quadratic\")\n",
    "print(f\"Cross-Validated QWK Score: {qwk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import make_scorer, cohen_kappa_score\n",
    "import numpy as np\n",
    "\n",
    "# Custom scorer for QWK\n",
    "def qwk_scorer(y_true, y_pred):\n",
    "    y_pred_rounded = np.rint(y_pred).astype(int)\n",
    "    return cohen_kappa_score(y_true, y_pred_rounded, weights=\"quadratic\")\n",
    "\n",
    "qwk_scorer = make_scorer(qwk_scorer, greater_is_better=True)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [5, 7, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [500, 1000],\n",
    "    'subsample': [0.6, 0.8],\n",
    "    'colsample_bytree': [0.7, 0.9],\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "model = XGBRegressor()\n",
    "\n",
    "# Initialize Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring=qwk_scorer,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit Grid Search\n",
    "grid_search.fit(X.to_pandas(), y_sii)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best QWK score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Random Search lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define parameter distributions\n",
    "param_dist = {\n",
    "    'max_depth': np.arange(3, 10),\n",
    "    'learning_rate': np.linspace(0.01, 0.2, 20),\n",
    "    'n_estimators': np.arange(100, 1100, 100),\n",
    "    'subsample': np.linspace(0.5, 1.0, 6),\n",
    "    'colsample_bytree': np.linspace(0.5, 1.0, 6),\n",
    "}\n",
    "\n",
    "# Initialize Random Search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,  # Number of parameter settings that are sampled\n",
    "    scoring=qwk_scorer,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit Random Search\n",
    "random_search.fit(X.to_pandas(), y_sii)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "print(\"Best QWK score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    model = XGBRegressor(**params)\n",
    "    score = cross_val_score(model, X.to_pandas(), y_sii, cv=5, scoring=qwk_scorer, n_jobs=-1).mean()\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# Define the search space\n",
    "space = {\n",
    "    'max_depth': hp.choice('max_depth', np.arange(3, 10, dtype=int)),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    'n_estimators': hp.choice('n_estimators', np.arange(100, 1100, 100, dtype=int)),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),\n",
    "}\n",
    "\n",
    "# Run optimization\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=50,\n",
    "    trials=trials,\n",
    "    rstate=np.random.RandomState(42)\n",
    ")\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters:\", best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Bayesian Optimization with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
    "    }\n",
    "    model = XGBRegressor(**params)\n",
    "    score = cross_val_score(model, X.to_pandas(), y_sii, cv=5, scoring=qwk_scorer, n_jobs=-1).mean()\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(\"Best parameters:\", study.best_params)\n",
    "print(\"Best QWK score:\", study.best_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "# Optimizing CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting catboost parameters\n",
    "params = dict(\n",
    "    loss_function=\"MultiRMSE\",\n",
    "    eval_metric=MultiTargetQWK(),\n",
    "    iterations=1 if IS_TEST else 100000,\n",
    "    learning_rate=0.1,\n",
    "    depth=5,\n",
    "    early_stopping_rounds=50,\n",
    ")\n",
    "o_params = {\n",
    "    \"n_classes\": 4,\n",
    "    \"n_trials\": 300\n",
    "}\n",
    "    \n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, cohen_kappa_score\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    model_params = {\n",
    "        'loss_function': \"MultiRMSE\",\n",
    "        'eval_metric': MultiTargetQWK(),\n",
    "        'iterations': 10000,\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
    "        'depth': trial.suggest_int('depth', 3, 10, step=1),\n",
    "        'early_stopping_rounds': trial.suggest_int('early_stopping_rounds', 40, 60, step=10),\n",
    "    }\n",
    "    optimizer_params = {\n",
    "        \"n_classes\": trial.suggest_int(\"n_classes\", 3, 7, step=1),\n",
    "        \"n_trials\": trial.suggest_int(\"n_trials\", 100, 400, step=50),\n",
    "    }\n",
    "    \n",
    "    model = CatBoostRegressor(**model_params)\n",
    "    \n",
    "    \n",
    "    def qwk_scorer(y_true, y_pred):\n",
    "        optimizer = OptimizedRounder(**optimizer_params)\n",
    "        optimizer.fit(y_pred, y_true)\n",
    "        y_pred_rounded = optimizer.predict(y_pred_total)\n",
    "        \n",
    "        #y_pred_rounded = np.rint(y_pred).astype(int)\n",
    "        return cohen_kappa_score(y_true, y_pred_rounded, weights=\"quadratic\")\n",
    "\n",
    "    qwk_scorer = make_scorer(qwk_scorer, greater_is_better=True)\n",
    "    \n",
    "    score = cross_val_score(model, X.to_pandas(), y_sii, cv=5, scoring=qwk_scorer, n_jobs=-1).mean()\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(\"Best parameters:\", study.best_params)\n",
    "print(\"Best QWK score:\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting catboost parameters\n",
    "params = dict(\n",
    "    loss_function=\"MultiRMSE\",\n",
    "    eval_metric=MultiTargetQWK(),\n",
    "    iterations=1 if IS_TEST else 100000,\n",
    "    learning_rate=0.1,\n",
    "    depth=5,\n",
    "    early_stopping_rounds=50,\n",
    ")\n",
    "o_params = {\n",
    "    \"n_classes\": 4,\n",
    "    \"n_trials\": 300\n",
    "}\n",
    "    \n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, cohen_kappa_score\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    model_params = {\n",
    "        'loss_function': \"MultiRMSE\",\n",
    "        'eval_metric': MultiTargetQWK(),\n",
    "        'iterations': 10000,\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
    "        'depth': trial.suggest_int('depth', 3, 10, step=1),\n",
    "        'early_stopping_rounds': trial.suggest_int('early_stopping_rounds', 40, 60, step=10),\n",
    "    }\n",
    "    '''optimizer_params = {\n",
    "        \"n_classes\": trial.suggest_int(\"n_classes\", 3, 7, step=1),\n",
    "        \"n_trials\": trial.suggest_int(\"n_trials\", 100, 400, step=50),\n",
    "    }'''\n",
    "    \n",
    "    model = CatBoostRegressor(**model_params)\n",
    "    \n",
    "    def qwk_scorer(y_true, y_pred):\n",
    "        #optimizer = OptimizedRounder(**optimizer_params)\n",
    "        #optimizer.fit(y_pred, y_true)\n",
    "        #y_pred_rounded = optimizer.predict(y_pred_total)\n",
    "        #y_pred_rounded = np.rint(y_pred).astype(int)\n",
    "        return cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n",
    "\n",
    "    qwk_scorer = make_scorer(qwk_scorer, greater_is_better=True)\n",
    "    \n",
    "    score = cross_val_score(model, X.to_pandas(), y_sii, cv=5, scoring=qwk_scorer, n_jobs=-1).mean()\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(\"Best parameters:\", study.best_params)\n",
    "print(\"Best QWK score:\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
