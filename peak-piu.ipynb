{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "papermill": {
     "duration": 3.303708,
     "end_time": "2024-09-20T23:27:53.232262",
     "exception": false,
     "start_time": "2024-09-20T23:27:49.928554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "\n",
    "from catboost import CatBoostRegressor, MultiTargetCustomMetric\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from numpy.typing import ArrayLike, NDArray\n",
    "from polars.testing import assert_frame_equal\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"Failed to optimize method\")\n",
    "\n",
    "\n",
    "#DATA_DIR = Path(\"/kaggle/input/child-mind-institute-problematic-internet-use\")\n",
    "DATA_DIR = Path(\"./data\")\n",
    "\n",
    "TARGET_COLS = [\n",
    "    \"PCIAT-PCIAT_01\",\n",
    "    \"PCIAT-PCIAT_02\",\n",
    "    \"PCIAT-PCIAT_03\",\n",
    "    \"PCIAT-PCIAT_04\",\n",
    "    \"PCIAT-PCIAT_05\",\n",
    "    \"PCIAT-PCIAT_06\",\n",
    "    \"PCIAT-PCIAT_07\",\n",
    "    \"PCIAT-PCIAT_08\",\n",
    "    \"PCIAT-PCIAT_09\",\n",
    "    \"PCIAT-PCIAT_10\",\n",
    "    \"PCIAT-PCIAT_11\",\n",
    "    \"PCIAT-PCIAT_12\",\n",
    "    \"PCIAT-PCIAT_13\",\n",
    "    \"PCIAT-PCIAT_14\",\n",
    "    \"PCIAT-PCIAT_15\",\n",
    "    \"PCIAT-PCIAT_16\",\n",
    "    \"PCIAT-PCIAT_17\",\n",
    "    \"PCIAT-PCIAT_18\",\n",
    "    \"PCIAT-PCIAT_19\",\n",
    "    \"PCIAT-PCIAT_20\",\n",
    "    \"PCIAT-PCIAT_Total\",\n",
    "    \"sii\",\n",
    "]\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    \"Basic_Demos-Age\",\n",
    "    \"PreInt_EduHx-computerinternet_hoursday\",\n",
    "    \"SDS-SDS_Total_Raw\",\n",
    "    \"Physical-Height\",\n",
    "    \"FGC-FGC_TL\",\n",
    "    \"Physical-Waist_Circumference\",\n",
    "    \"BIA-BIA_FMI\",\n",
    "    \"BIA-BIA_Fat\",\n",
    "    \"Fitness_Endurance-Time_Sec\",\n",
    "    \"Fitness_Endurance-Max_Stage\",\n",
    "    \"FGC-FGC_CU_Zone\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "papermill": {
     "duration": 0.202543,
     "end_time": "2024-09-20T23:27:53.443436",
     "exception": false,
     "start_time": "2024-09-20T23:27:53.240893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pl.read_csv(DATA_DIR / \"train.csv\")\n",
    "test = pl.read_csv(DATA_DIR / \"test.csv\")\n",
    "train_test = pl.concat([train, test], how=\"diagonal\")\n",
    "\n",
    "IS_TEST = test.height <= 100\n",
    "\n",
    "assert_frame_equal(train, train_test[: train.height].select(train.columns))\n",
    "assert_frame_equal(test, train_test[train.height :].select(test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "papermill": {
     "duration": 0.066498,
     "end_time": "2024-09-20T23:27:53.518169",
     "exception": false,
     "start_time": "2024-09-20T23:27:53.451671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cast string columns to categorical\n",
    "train_test = train_test.with_columns(cs.string().cast(pl.Categorical).fill_null(\"NAN\"))\n",
    "train = train_test[: train.height]\n",
    "test = train_test[train.height :]\n",
    "\n",
    "# ignore rows with null values in TARGET_COLS\n",
    "train_without_null = train_test.drop_nulls(subset=TARGET_COLS)\n",
    "X = train_without_null.select(FEATURE_COLS)\n",
    "X_test = test.select(FEATURE_COLS)\n",
    "y = train_without_null.select(TARGET_COLS)\n",
    "y_sii = y.get_column(\"sii\").to_numpy()  # ground truth\n",
    "cat_features = X.select(cs.categorical()).columns\n",
    "\n",
    "print(\"Features selected:\")\n",
    "print(cat_features)  # Should be none"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "papermill": {
     "duration": 0.008025,
     "end_time": "2024-09-20T23:27:53.534860",
     "exception": false,
     "start_time": "2024-09-20T23:27:53.526835",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tubotubo's Quadratic Weighted Kappa metric & Optuna optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "papermill": {
     "duration": 0.03706,
     "end_time": "2024-09-20T23:27:53.580139",
     "exception": false,
     "start_time": "2024-09-20T23:27:53.543079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiTargetQWK(MultiTargetCustomMetric):\n",
    "    def get_final_error(self, error, weight):\n",
    "        return np.sum(error)  # / np.sum(weight)\n",
    "\n",
    "    def is_max_optimal(self):\n",
    "        # if True, the bigger the better\n",
    "        return True\n",
    "\n",
    "    def evaluate(self, approxes, targets, weight):\n",
    "        # approxes: 予測値 (shape: [ターゲット数, サンプル数])\n",
    "        # targets: 実際の値 (shape: [ターゲット数, サンプル数])\n",
    "        # weight: サンプルごとの重み (Noneも可)\n",
    "\n",
    "        approx = np.clip(approxes[-1], 0, 3).round().astype(int)\n",
    "        target = targets[-1]\n",
    "\n",
    "        qwk = cohen_kappa_score(target, approx, weights=\"quadratic\")\n",
    "\n",
    "        return qwk, 1\n",
    "\n",
    "    def get_custom_metric_name(self):\n",
    "        return \"MultiTargetQWK\"\n",
    "\n",
    "\n",
    "class OptimizedRounder:\n",
    "    \"\"\"\n",
    "    A class for optimizing the rounding of continuous predictions into discrete class labels using Optuna.\n",
    "    The optimization process maximizes the Quadratic Weighted Kappa score by learning thresholds that separate\n",
    "    continuous predictions into class intervals.\n",
    "\n",
    "    Args:\n",
    "        n_classes (int): The number of discrete class labels.\n",
    "        n_trials (int, optional): The number of trials for the Optuna optimization. Defaults to 100.\n",
    "\n",
    "    Attributes:\n",
    "        n_classes (int): The number of discrete class labels.\n",
    "        labels (NDArray[np.int_]): An array of class labels from 0 to `n_classes - 1`.\n",
    "        n_trials (int): The number of optimization trials.\n",
    "        metric (Callable): The Quadratic Weighted Kappa score metric used for optimization.\n",
    "        thresholds (List[float]): The optimized thresholds learned after calling `fit()`.\n",
    "\n",
    "    Methods:\n",
    "        fit(y_pred: NDArray[np.float_], y_true: NDArray[np.int_]) -> None:\n",
    "            Fits the rounding thresholds based on continuous predictions and ground truth labels.\n",
    "\n",
    "            Args:\n",
    "                y_pred (NDArray[np.float_]): Continuous predictions that need to be rounded.\n",
    "                y_true (NDArray[np.int_]): Ground truth class labels.\n",
    "\n",
    "            Returns:\n",
    "                None\n",
    "\n",
    "        predict(y_pred: NDArray[np.float_]) -> NDArray[np.int_]:\n",
    "            Predicts discrete class labels by rounding continuous predictions using the fitted thresholds.\n",
    "            `fit()` must be called before `predict()`.\n",
    "\n",
    "            Args:\n",
    "                y_pred (NDArray[np.float_]): Continuous predictions to be rounded.\n",
    "\n",
    "            Returns:\n",
    "                NDArray[np.int_]: Predicted class labels.\n",
    "\n",
    "        _normalize(y: NDArray[np.float_]) -> NDArray[np.float_]:\n",
    "            Normalizes the continuous values to the range [0, `n_classes - 1`].\n",
    "\n",
    "            Args:\n",
    "                y (NDArray[np.float_]): Continuous values to be normalized.\n",
    "\n",
    "            Returns:\n",
    "                NDArray[np.float_]: Normalized values.\n",
    "\n",
    "    References:\n",
    "        - This implementation uses Optuna for threshold optimization.\n",
    "        - Quadratic Weighted Kappa is used as the evaluation metric.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_classes: int, n_trials: int = 100):\n",
    "        self.n_classes = n_classes\n",
    "        self.labels = np.arange(n_classes)\n",
    "        self.n_trials = n_trials\n",
    "        self.metric = partial(cohen_kappa_score, weights=\"quadratic\")\n",
    "\n",
    "    def fit(self, y_pred: NDArray[np.float_], y_true: NDArray[np.int_]) -> None:\n",
    "        y_pred = self._normalize(y_pred)\n",
    "\n",
    "        def objective(trial: optuna.Trial) -> float:\n",
    "            thresholds = []\n",
    "            for i in range(self.n_classes - 1):\n",
    "                low = max(thresholds) if i > 0 else min(self.labels)\n",
    "                high = max(self.labels)\n",
    "                th = trial.suggest_float(f\"threshold_{i}\", low, high)\n",
    "                thresholds.append(th)\n",
    "            try:\n",
    "                y_pred_rounded = np.digitize(y_pred, thresholds)\n",
    "            except ValueError:\n",
    "                return -100\n",
    "            return self.metric(y_true, y_pred_rounded)\n",
    "\n",
    "        optuna.logging.disable_default_handler()\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(\n",
    "            objective,\n",
    "            n_trials=self.n_trials,\n",
    "        )\n",
    "        self.thresholds = [study.best_params[f\"threshold_{i}\"] for i in range(self.n_classes - 1)]\n",
    "\n",
    "    def predict(self, y_pred: NDArray[np.float_]) -> NDArray[np.int_]:\n",
    "        assert hasattr(self, \"thresholds\"), \"fit() must be called before predict()\"\n",
    "        y_pred = self._normalize(y_pred)\n",
    "        return np.digitize(y_pred, self.thresholds)\n",
    "\n",
    "    def _normalize(self, y: NDArray[np.float_]) -> NDArray[np.float_]:\n",
    "        # normalize y_pred to [0, n_classes - 1]\n",
    "        return (y - y.min()) / (y.max() - y.min()) * (self.n_classes - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "papermill": {
     "duration": 0.008373,
     "end_time": "2024-09-20T23:27:53.597003",
     "exception": false,
     "start_time": "2024-09-20T23:27:53.588630",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Start making models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "papermill": {
     "duration": 0.08071,
     "end_time": "2024-09-20T23:27:53.899286",
     "exception": false,
     "start_time": "2024-09-20T23:27:53.818576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Updated SILLY MODEL with RFE\n",
    "class SillyManRFE():\n",
    "    def __init__(self, n_features_to_select=10):\n",
    "        self.selector = RFE(estimator=CatBoostRegressor(**params), n_features_to_select=n_features_to_select, step=1)\n",
    "        self.cat = CatBoostRegressor(**params)\n",
    "        \n",
    "    def fit(self, x, y, eval_set, cat_features, verbose=False):\n",
    "        self.cat_features = cat_features\n",
    "        \n",
    "        # Apply RFE for feature selection\n",
    "        X_selected = self.selector.fit_transform(x, y)\n",
    "        \n",
    "        # Identify selected feature indices\n",
    "        selected_features = np.array(x.columns)[self.selector.support_]\n",
    "        \n",
    "        # Identify categorical features after selection\n",
    "        new_cat_features = [i for i, col in enumerate(x.columns) if col in cat_features and col in selected_features]\n",
    "        \n",
    "        # Prepare evaluation set\n",
    "        X_val_selected = self.selector.transform(eval_set[0])\n",
    "        \n",
    "        # Fit CatBoost\n",
    "        self.cat.fit(\n",
    "            X_selected,\n",
    "            y,\n",
    "            eval_set=(X_val_selected, eval_set[1]),\n",
    "            cat_features=new_cat_features,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "    def predict(self, x):\n",
    "        X_selected = self.selector.transform(x)\n",
    "        return self.cat.predict(X_selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "papermill": {
     "duration": 0.040654,
     "end_time": "2024-09-20T23:27:54.646588",
     "exception": false,
     "start_time": "2024-09-20T23:27:54.605934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, SplineTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from catboost import CatBoostRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define your parameters\n",
    "params = dict(\n",
    "    loss_function=\"MultiRMSE\",\n",
    "    eval_metric=\"MultiRMSE\",  # Ensure eval_metric is appropriate\n",
    "    iterations=7000,  # Adjust as needed\n",
    "    learning_rate=0.05,\n",
    "    depth=5,\n",
    "    early_stopping_rounds=50,\n",
    ")\n",
    "\n",
    "# Updated SILLY MODEL\n",
    "class SillyManPolynomial:\n",
    "    def __init__(\n",
    "        self,\n",
    "        include_bias=False,\n",
    "        imputation_strategy='mean',\n",
    "        use_spline=True,\n",
    "        use_poly=False,\n",
    "        use_raw=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the SillyManPolynomial model with options to include Spline, Polynomial, and/or raw features.\n",
    "\n",
    "        Parameters:\n",
    "        - degree (int): Degree of the polynomial features.\n",
    "        - include_bias (bool): Whether to include a bias column.\n",
    "        - imputation_strategy (str): Strategy for imputing missing values ('mean', 'median', 'most_frequent', etc.).\n",
    "        - use_spline (bool): Whether to include Spline transformed features.\n",
    "        - use_poly (bool): Whether to include Polynomial transformed features.\n",
    "        - use_raw (bool): Whether to include raw numerical features.\n",
    "        \"\"\"\n",
    "        self.use_spline = use_spline\n",
    "        self.use_poly = use_poly\n",
    "        self.use_raw = use_raw\n",
    "\n",
    "        self.imputer = SimpleImputer(strategy=imputation_strategy)\n",
    "        if self.use_poly:\n",
    "            self.poly = PolynomialFeatures(degree=2, include_bias=include_bias)\n",
    "        if self.use_spline:\n",
    "            self.spline = SplineTransformer(n_knots=5, degree=3, include_bias=include_bias)\n",
    "        self.cat = CatBoostRegressor(**params)\n",
    "\n",
    "    def fit(self, x, y, eval_set, cat_features, verbose=False):\n",
    "        \"\"\"\n",
    "        Fits the model with selected feature transformations and imputation.\n",
    "\n",
    "        Parameters:\n",
    "        - x (pd.DataFrame): Training features.\n",
    "        - y (pd.Series or pd.DataFrame): Training target.\n",
    "        - eval_set (tuple): Validation set as (X_val, y_val).\n",
    "        - cat_features (list): List of categorical feature names.\n",
    "        - verbose (bool): Verbosity flag.\n",
    "        \"\"\"\n",
    "        self.cat_features = cat_features\n",
    "\n",
    "        # Separate numerical and categorical features\n",
    "        if cat_features:\n",
    "            numerical = x.drop(columns=self.cat_features)\n",
    "            categorical = x[self.cat_features]\n",
    "        else:\n",
    "            numerical = x.copy()\n",
    "            categorical = None\n",
    "\n",
    "        # Impute missing values in numerical features\n",
    "        X_train_imputed = self.imputer.fit_transform(numerical)\n",
    "        X_val_imputed = self.imputer.transform(\n",
    "            eval_set[0].drop(columns=self.cat_features) if self.cat_features else eval_set[0]\n",
    "        )\n",
    "\n",
    "        # Initialize lists to hold transformed features\n",
    "        X_train_features = []\n",
    "        X_val_features = []\n",
    "\n",
    "        # Apply Spline Transformation if selected\n",
    "        if self.use_spline:\n",
    "            X_train_spline = self.spline.fit_transform(X_train_imputed)\n",
    "            X_val_spline = self.spline.transform(X_val_imputed)\n",
    "            spline_feature_names = self.spline.get_feature_names_out(input_features=numerical.columns)\n",
    "            # Prefix the feature names to avoid duplicates\n",
    "            spline_feature_names = ['spline_' + name for name in spline_feature_names]\n",
    "            X_train_features.append(pd.DataFrame(X_train_spline, columns=spline_feature_names))\n",
    "            X_val_features.append(pd.DataFrame(X_val_spline, columns=spline_feature_names))\n",
    "\n",
    "        # Apply Polynomial Transformation if selected\n",
    "        if self.use_poly:\n",
    "            X_train_poly = self.poly.fit_transform(X_train_imputed)\n",
    "            X_val_poly = self.poly.transform(X_val_imputed)\n",
    "            poly_feature_names = self.poly.get_feature_names_out(input_features=numerical.columns)\n",
    "            # Prefix the feature names to avoid duplicates\n",
    "            poly_feature_names = ['poly_' + name for name in poly_feature_names]\n",
    "            X_train_features.append(pd.DataFrame(X_train_poly, columns=poly_feature_names))\n",
    "            X_val_features.append(pd.DataFrame(X_val_poly, columns=poly_feature_names))\n",
    "\n",
    "        # Include raw numerical features if selected\n",
    "        if self.use_raw:\n",
    "            X_train_raw = pd.DataFrame(X_train_imputed, columns=numerical.columns)\n",
    "            X_val_raw = pd.DataFrame(X_val_imputed, columns=numerical.columns)\n",
    "            # Prefix the feature names to avoid duplicates\n",
    "            raw_feature_names = ['raw_' + name for name in numerical.columns]\n",
    "            X_train_raw.columns = raw_feature_names\n",
    "            X_val_raw.columns = raw_feature_names\n",
    "            X_train_features.append(X_train_raw)\n",
    "            X_val_features.append(X_val_raw)\n",
    "\n",
    "        # Check if at least one feature set is included\n",
    "        if not X_train_features:\n",
    "            raise ValueError(\"At least one of use_spline, use_poly, or use_raw must be True.\")\n",
    "\n",
    "        # Concatenate all selected features\n",
    "        X_train_processed = pd.concat(X_train_features, axis=1).reset_index(drop=True)\n",
    "        X_val_processed = pd.concat(X_val_features, axis=1).reset_index(drop=True)\n",
    "\n",
    "        # Append categorical features if any\n",
    "        if categorical is not None:\n",
    "            X_train_cat = categorical.reset_index(drop=True)\n",
    "            X_val_cat = eval_set[0][self.cat_features].reset_index(drop=True)\n",
    "            X_train_processed = pd.concat([X_train_processed, X_train_cat], axis=1)\n",
    "            X_val_processed = pd.concat([X_val_processed, X_val_cat], axis=1)\n",
    "            # Update cat_features indices to point to the categorical features appended at the end\n",
    "            new_cat_features = list(range(\n",
    "                X_train_processed.shape[1] - len(self.cat_features),\n",
    "                X_train_processed.shape[1]\n",
    "            ))\n",
    "        else:\n",
    "            new_cat_features = []\n",
    "\n",
    "        # Fit CatBoost\n",
    "        self.cat.fit(\n",
    "            X_train_processed,\n",
    "            y,\n",
    "            eval_set=(X_val_processed, eval_set[1]),\n",
    "            cat_features=new_cat_features,\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predicts using the fitted model.\n",
    "\n",
    "        Parameters:\n",
    "        - x (pd.DataFrame): Features for prediction.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: Predictions.\n",
    "        \"\"\"\n",
    "        # Separate numerical and categorical features\n",
    "        if self.cat_features:\n",
    "            numerical = x.drop(columns=self.cat_features)\n",
    "            categorical = x[self.cat_features]\n",
    "        else:\n",
    "            numerical = x.copy()\n",
    "            categorical = None\n",
    "\n",
    "        # Impute missing values\n",
    "        X_imputed = self.imputer.transform(numerical)\n",
    "\n",
    "        # Initialize list to hold transformed features\n",
    "        X_features = []\n",
    "\n",
    "        # Apply Spline Transformation if selected\n",
    "        if self.use_spline:\n",
    "            X_spline = self.spline.transform(X_imputed)\n",
    "            spline_feature_names = self.spline.get_feature_names_out(input_features=numerical.columns)\n",
    "            spline_feature_names = ['spline_' + name for name in spline_feature_names]\n",
    "            X_features.append(pd.DataFrame(X_spline, columns=spline_feature_names))\n",
    "\n",
    "        # Apply Polynomial Transformation if selected\n",
    "        if self.use_poly:\n",
    "            X_poly = self.poly.transform(X_imputed)\n",
    "            poly_feature_names = self.poly.get_feature_names_out(input_features=numerical.columns)\n",
    "            poly_feature_names = ['poly_' + name for name in poly_feature_names]\n",
    "            X_features.append(pd.DataFrame(X_poly, columns=poly_feature_names))\n",
    "\n",
    "        # Include raw numerical features if selected\n",
    "        if self.use_raw:\n",
    "            X_raw = pd.DataFrame(X_imputed, columns=numerical.columns)\n",
    "            raw_feature_names = ['raw_' + name for name in numerical.columns]\n",
    "            X_raw.columns = raw_feature_names\n",
    "            X_features.append(X_raw)\n",
    "\n",
    "        # Check if at least one feature set is included\n",
    "        if not X_features:\n",
    "            raise ValueError(\"At least one of use_spline, use_poly, or use_raw must be True.\")\n",
    "\n",
    "        # Concatenate all selected features\n",
    "        X_processed = pd.concat(X_features, axis=1).reset_index(drop=True)\n",
    "\n",
    "        # Append categorical features if any\n",
    "        if categorical is not None:\n",
    "            X_cat = categorical.reset_index(drop=True)\n",
    "            X_processed = pd.concat([X_processed, X_cat.reset_index(drop=True)], axis=1)\n",
    "\n",
    "        return self.cat.predict(X_processed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import (\n",
    "    PolynomialFeatures,\n",
    "    SplineTransformer,\n",
    "    StandardScaler,\n",
    "    MinMaxScaler,\n",
    "    QuantileTransformer\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define your parameters\n",
    "catboost_params = dict(\n",
    "    loss_function=\"MultiRMSE\",\n",
    "    eval_metric=\"MultiRMSE\",\n",
    "    iterations=7000,\n",
    "    learning_rate=0.05,\n",
    "    depth=5,\n",
    "    early_stopping_rounds=50,\n",
    ")\n",
    "\n",
    "xgboost_params = dict(\n",
    "    objective=\"reg:squarederror\",\n",
    "    eval_metric=\"rmse\",\n",
    "    n_estimators=7000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    early_stopping_rounds=50,\n",
    "    tree_method='hist'  # or 'auto' depending on your setup\n",
    ")\n",
    "\n",
    "class SillyManCombinational:\n",
    "    def __init__(\n",
    "        self,\n",
    "        include_bias=False,\n",
    "        imputation_strategy='mean',\n",
    "        use_spline=True,\n",
    "        use_poly=False,\n",
    "        use_raw=False,\n",
    "        use_standard_scaler=False,\n",
    "        use_minmax_scaler=False,\n",
    "        use_quantile_transformer=False,\n",
    "        base_estimators=['catboost'],  # options: ['catboost', 'xgboost']\n",
    "        final_estimators=['catboost'],  # options: ['catboost', 'xgboost']\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the SillyManPolynomial model with options to include various transformers and base estimators.\n",
    "\n",
    "        Parameters:\n",
    "        - include_bias (bool): Whether to include a bias column.\n",
    "        - imputation_strategy (str): Strategy for imputing missing values.\n",
    "        - use_spline (bool): Whether to include Spline transformed features.\n",
    "        - use_poly (bool): Whether to include Polynomial transformed features.\n",
    "        - use_raw (bool): Whether to include raw numerical features.\n",
    "        - use_standard_scaler (bool): Whether to include StandardScaler transformed features.\n",
    "        - use_minmax_scaler (bool): Whether to include MinMaxScaler transformed features.\n",
    "        - use_quantile_transformer (bool): Whether to include QuantileTransformer transformed features.\n",
    "        - base_estimators (list): List of estimators to use for base models ['catboost', 'xgboost'].\n",
    "        - final_estimators (list): List of estimators to use for final model ['catboost', 'xgboost'].\n",
    "        \"\"\"\n",
    "        # Transformer toggles\n",
    "        self.use_spline = use_spline\n",
    "        self.use_poly = use_poly\n",
    "        self.use_raw = use_raw\n",
    "        self.use_standard_scaler = use_standard_scaler\n",
    "        self.use_minmax_scaler = use_minmax_scaler\n",
    "        self.use_quantile_transformer = use_quantile_transformer\n",
    "\n",
    "        # Estimator types\n",
    "        self.base_estimators_types = base_estimators\n",
    "        self.final_estimators_types = final_estimators\n",
    "\n",
    "        self.imputer = SimpleImputer(strategy=imputation_strategy)\n",
    "        if self.use_poly:\n",
    "            self.poly = PolynomialFeatures(degree=2, include_bias=include_bias)\n",
    "        if self.use_spline:\n",
    "            self.spline = SplineTransformer(n_knots=5, degree=3, include_bias=include_bias)\n",
    "        if self.use_standard_scaler:\n",
    "            self.standard_scaler = StandardScaler()\n",
    "        if self.use_minmax_scaler:\n",
    "            self.minmax_scaler = MinMaxScaler()\n",
    "        if self.use_quantile_transformer:\n",
    "            self.quantile_transformer = QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "        # Base estimators dict\n",
    "        self.base_estimators = {}\n",
    "        # Final estimator(s)\n",
    "        self.final_estimators = {}\n",
    "\n",
    "    def fit(self, x, y, eval_set, cat_features, verbose=False):\n",
    "        \"\"\"\n",
    "        Fits the model with selected feature transformations and imputation.\n",
    "\n",
    "        Parameters:\n",
    "        - x (pd.DataFrame): Training features.\n",
    "        - y (pd.Series or pd.DataFrame): Training target.\n",
    "        - eval_set (tuple): Validation set as (X_val, y_val).\n",
    "        - cat_features (list): List of categorical feature names.\n",
    "        - verbose (bool): Verbosity flag.\n",
    "        \"\"\"\n",
    "        self.cat_features = cat_features\n",
    "\n",
    "        # Separate numerical and categorical features\n",
    "        if cat_features:\n",
    "            numerical = x.drop(columns=self.cat_features)\n",
    "            categorical = x[self.cat_features]\n",
    "        else:\n",
    "            numerical = x.copy()\n",
    "            categorical = None\n",
    "\n",
    "        # Impute missing values in numerical features\n",
    "        X_train_imputed = self.imputer.fit_transform(numerical)\n",
    "        X_val_imputed = self.imputer.transform(\n",
    "            eval_set[0].drop(columns=self.cat_features) if self.cat_features else eval_set[0]\n",
    "        )\n",
    "\n",
    "        # Initialize list to collect base estimator predictions\n",
    "        base_train_preds = []\n",
    "        base_val_preds = []\n",
    "\n",
    "        # Transformer functions mapping\n",
    "        transformers = {}\n",
    "        if self.use_spline:\n",
    "            transformers['spline'] = self.spline\n",
    "        if self.use_poly:\n",
    "            transformers['poly'] = self.poly\n",
    "        if self.use_raw:\n",
    "            transformers['raw'] = None  # raw data, no transformer\n",
    "        if self.use_standard_scaler:\n",
    "            transformers['standard_scaler'] = self.standard_scaler\n",
    "        if self.use_minmax_scaler:\n",
    "            transformers['minmax_scaler'] = self.minmax_scaler\n",
    "        if self.use_quantile_transformer:\n",
    "            transformers['quantile_transformer'] = self.quantile_transformer\n",
    "\n",
    "        if not transformers:\n",
    "            raise ValueError(\"At least one transformer must be selected.\")\n",
    "\n",
    "        # For each transformer, apply transformation, train base estimators\n",
    "        for name, transformer in transformers.items():\n",
    "            if transformer is not None:\n",
    "                # Fit transformer on training data\n",
    "                X_train_transformed = transformer.fit_transform(X_train_imputed)\n",
    "                X_val_transformed = transformer.transform(X_val_imputed)\n",
    "                feature_names = transformer.get_feature_names_out(input_features=numerical.columns)\n",
    "                feature_names = [f'{name}_{feat}' for feat in feature_names]\n",
    "            else:\n",
    "                # Raw data\n",
    "                X_train_transformed = X_train_imputed\n",
    "                X_val_transformed = X_val_imputed\n",
    "                feature_names = [f'raw_{feat}' for feat in numerical.columns]\n",
    "\n",
    "            # Create DataFrames\n",
    "            X_train_df = pd.DataFrame(X_train_transformed, columns=feature_names)\n",
    "            X_val_df = pd.DataFrame(X_val_transformed, columns=feature_names)\n",
    "\n",
    "            # Append categorical features if any\n",
    "            if categorical is not None:\n",
    "                X_train_cat = categorical.reset_index(drop=True)\n",
    "                X_val_cat = eval_set[0][self.cat_features].reset_index(drop=True)\n",
    "                X_train_df = pd.concat([X_train_df.reset_index(drop=True), X_train_cat], axis=1)\n",
    "                X_val_df = pd.concat([X_val_df.reset_index(drop=True), X_val_cat], axis=1)\n",
    "                # Update cat_features indices to point to the categorical features appended at the end\n",
    "                new_cat_features = list(range(\n",
    "                    X_train_df.shape[1] - len(self.cat_features),\n",
    "                    X_train_df.shape[1]\n",
    "                ))\n",
    "            else:\n",
    "                new_cat_features = []\n",
    "\n",
    "            # Train base estimators\n",
    "            for est_type in self.base_estimators_types:\n",
    "                est_name = f'{name}_{est_type}'\n",
    "                if est_type == 'catboost':\n",
    "                    estimator = CatBoostRegressor(**catboost_params)\n",
    "                    estimator.fit(\n",
    "                        X_train_df,\n",
    "                        y,\n",
    "                        eval_set=(X_val_df, eval_set[1]),\n",
    "                        cat_features=new_cat_features,\n",
    "                        verbose=verbose\n",
    "                    )\n",
    "                elif est_type == 'xgboost':\n",
    "                    estimator = XGBRegressor(**xgboost_params)\n",
    "                    estimator.fit(\n",
    "                        X_train_df,\n",
    "                        y,\n",
    "                        eval_set=[(X_val_df, eval_set[1])],\n",
    "                        verbose=verbose\n",
    "                    )\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported estimator type: {est_type}\")\n",
    "                # Save estimator\n",
    "                self.base_estimators[est_name] = estimator\n",
    "\n",
    "                # Collect predictions\n",
    "                train_pred = estimator.predict(X_train_df)\n",
    "                val_pred = estimator.predict(X_val_df)\n",
    "\n",
    "                # Ensure predictions are 2D arrays\n",
    "                if train_pred.ndim == 1:\n",
    "                    train_pred = train_pred.reshape(-1, 1)\n",
    "                    val_pred = val_pred.reshape(-1, 1)\n",
    "\n",
    "                # Generate appropriate column names for multi-output\n",
    "                n_outputs = train_pred.shape[1]\n",
    "                if n_outputs == 1:\n",
    "                    columns = [est_name]\n",
    "                else:\n",
    "                    columns = [f'{est_name}_output_{i}' for i in range(n_outputs)]\n",
    "\n",
    "                # Create DataFrames with correct column names\n",
    "                base_train_preds.append(pd.DataFrame(train_pred, columns=columns))\n",
    "                base_val_preds.append(pd.DataFrame(val_pred, columns=columns))\n",
    "\n",
    "        # Combine base estimator predictions\n",
    "        X_train_meta = pd.concat(base_train_preds, axis=1).reset_index(drop=True)\n",
    "        X_val_meta = pd.concat(base_val_preds, axis=1).reset_index(drop=True)\n",
    "\n",
    "        # Train final estimator(s)\n",
    "        for est_type in self.final_estimators_types:\n",
    "            est_name = f'final_{est_type}'\n",
    "            if est_type == 'catboost':\n",
    "                final_estimator = CatBoostRegressor(**catboost_params)\n",
    "                final_estimator.fit(\n",
    "                    X_train_meta,\n",
    "                    y,\n",
    "                    eval_set=(X_val_meta, eval_set[1]),\n",
    "                    verbose=verbose\n",
    "                )\n",
    "            elif est_type == 'xgboost':\n",
    "                final_estimator = XGBRegressor(**xgboost_params)\n",
    "                final_estimator.fit(\n",
    "                    X_train_meta,\n",
    "                    y,\n",
    "                    eval_set=[(X_val_meta, eval_set[1])],\n",
    "                    verbose=verbose\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported estimator type: {est_type}\")\n",
    "            # Save final estimator\n",
    "            self.final_estimators[est_type] = final_estimator\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predicts using the fitted model.\n",
    "\n",
    "        Parameters:\n",
    "        - x (pd.DataFrame): Features for prediction.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: Predictions.\n",
    "        \"\"\"\n",
    "        # Separate numerical and categorical features\n",
    "        if self.cat_features:\n",
    "            numerical = x.drop(columns=self.cat_features)\n",
    "            categorical = x[self.cat_features]\n",
    "        else:\n",
    "            numerical = x.copy()\n",
    "            categorical = None\n",
    "\n",
    "        # Impute missing values\n",
    "        X_imputed = self.imputer.transform(numerical)\n",
    "\n",
    "        # Initialize list to collect base estimator predictions\n",
    "        base_preds = []\n",
    "\n",
    "        # Transformer functions mapping (same as in fit)\n",
    "        transformers = {}\n",
    "        if self.use_spline:\n",
    "            transformers['spline'] = self.spline\n",
    "        if self.use_poly:\n",
    "            transformers['poly'] = self.poly\n",
    "        if self.use_raw:\n",
    "            transformers['raw'] = None  # raw data, no transformer\n",
    "        if self.use_standard_scaler:\n",
    "            transformers['standard_scaler'] = self.standard_scaler\n",
    "        if self.use_minmax_scaler:\n",
    "            transformers['minmax_scaler'] = self.minmax_scaler\n",
    "        if self.use_quantile_transformer:\n",
    "            transformers['quantile_transformer'] = self.quantile_transformer\n",
    "\n",
    "        # For each transformer, apply transformation, get base estimator predictions\n",
    "        for name, transformer in transformers.items():\n",
    "            if transformer is not None:\n",
    "                X_transformed = transformer.transform(X_imputed)\n",
    "                feature_names = transformer.get_feature_names_out(input_features=numerical.columns)\n",
    "                feature_names = [f'{name}_{feat}' for feat in feature_names]\n",
    "            else:\n",
    "                X_transformed = X_imputed\n",
    "                feature_names = [f'raw_{feat}' for feat in numerical.columns]\n",
    "\n",
    "            # Create DataFrame\n",
    "            X_df = pd.DataFrame(X_transformed, columns=feature_names)\n",
    "\n",
    "            # Append categorical features if any\n",
    "            if categorical is not None:\n",
    "                X_cat = categorical.reset_index(drop=True)\n",
    "                X_df = pd.concat([X_df.reset_index(drop=True), X_cat.reset_index(drop=True)], axis=1)\n",
    "\n",
    "            # Get base estimator predictions\n",
    "            for est_type in self.base_estimators_types:\n",
    "                est_name = f'{name}_{est_type}'\n",
    "                estimator = self.base_estimators[est_name]\n",
    "                pred = estimator.predict(X_df)\n",
    "                if pred.ndim == 1:\n",
    "                    pred = pred.reshape(-1, 1)\n",
    "\n",
    "                # Generate appropriate column names for multi-output\n",
    "                n_outputs = pred.shape[1]\n",
    "                if n_outputs == 1:\n",
    "                    columns = [est_name]\n",
    "                else:\n",
    "                    columns = [f'{est_name}_output_{i}' for i in range(n_outputs)]\n",
    "\n",
    "                base_preds.append(pd.DataFrame(pred, columns=columns))\n",
    "\n",
    "        # Combine base estimator predictions\n",
    "        X_meta = pd.concat(base_preds, axis=1).reset_index(drop=True)\n",
    "\n",
    "        # Collect final predictions\n",
    "        final_preds = []\n",
    "        for est_type in self.final_estimators_types:\n",
    "            final_estimator = self.final_estimators[est_type]\n",
    "            pred = final_estimator.predict(X_meta)\n",
    "            final_preds.append(pred)\n",
    "\n",
    "        # Average predictions if multiple final estimators\n",
    "        if len(final_preds) == 1:\n",
    "            return final_preds[0]\n",
    "        else:\n",
    "            return np.mean(final_preds, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "papermill": {
     "duration": 66.117441,
     "end_time": "2024-09-20T23:29:00.772381",
     "exception": false,
     "start_time": "2024-09-20T23:27:54.654940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=52)\n",
    "models: list = []\n",
    "y_pred = np.full((X.height, len(TARGET_COLS)), fill_value=np.nan)\n",
    "\n",
    "for train_idx, val_idx in skf.split(X, y_sii):\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Initialize and train model\n",
    "    model = SillyManCombinational(\n",
    "        use_spline=True,\n",
    "        use_poly=False,\n",
    "        use_raw=False,\n",
    "        use_standard_scaler=False,\n",
    "        base_estimators=['catboost'],\n",
    "        final_estimators=['catboost']\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train.to_pandas(),\n",
    "        y_train.to_pandas(),\n",
    "        eval_set=(X_val.to_pandas(), y_val.to_pandas()),\n",
    "        cat_features=cat_features,\n",
    "        verbose=False,\n",
    "    )\n",
    "    models.append(model)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred[val_idx] = model.predict(X_val.to_pandas())\n",
    "\n",
    "assert np.isnan(y_pred).sum() == 0\n",
    "\n",
    "# Optimize thresholds\n",
    "optimizer = OptimizedRounder(n_classes=4, n_trials=300)\n",
    "y_pred_total = y_pred[:, TARGET_COLS.index(\"PCIAT-PCIAT_Total\")]\n",
    "optimizer.fit(y_pred_total, y_sii)\n",
    "y_pred_rounded = optimizer.predict(y_pred_total)\n",
    "\n",
    "# Calculate QWK\n",
    "qwk = cohen_kappa_score(y_sii, y_pred_rounded, weights=\"quadratic\")\n",
    "print(f\"Cross-Validated QWK Score: {qwk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Combinational Model V2!\n",
    "Base                    - Cross-Validated QWK Score: 0.4152442719436863\n",
    "\n",
    "    model = SillyManCombinational(\n",
    "        use_spline=True,\n",
    "        use_poly=True,\n",
    "        use_raw=True,\n",
    "        use_standard_scaler=True,\n",
    "        base_estimators=['catboost', 'xgboost'],\n",
    "        final_estimators=['catboost', 'xgboost']\n",
    "    ) \n",
    "\n",
    "Base - No Scaler        - Cross-Validated QWK Score: 0.4155735952333438\n",
    "\n",
    "    model = SillyManCombinational(\n",
    "        use_spline=True,\n",
    "        use_poly=True,\n",
    "        use_raw=True,\n",
    "        use_standard_scaler=False,\n",
    "        base_estimators=['catboost', 'xgboost'],\n",
    "        final_estimators=['catboost', 'xgboost']\n",
    "    )\n",
    "\n",
    "CatBoost - No Scaler    - Cross-Validated QWK Score: 0.4563740524739843\n",
    "\n",
    "    model = SillyManCombinational(\n",
    "        use_spline=True,\n",
    "        use_poly=True,\n",
    "        use_raw=True,\n",
    "        use_standard_scaler=False,\n",
    "        base_estimators=['catboost'],\n",
    "        final_estimators=['catboost']\n",
    "    )\n",
    "CatBoost - No Scaler, Spline -  Cross-Validated QWK Score: 0.45752648926425\n",
    "\n",
    "    model = SillyManCombinational(\n",
    "        use_spline=True,\n",
    "        use_poly=False,\n",
    "        use_raw=False,\n",
    "        use_standard_scaler=False,\n",
    "        base_estimators=['catboost'],\n",
    "        final_estimators=['catboost']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Combinational Model!\n",
    "Spline  - Cross-Validated QWK Score: 0.46978673137433913\n",
    "Poly    - Cross-Validated QWK Score: 0.4603240623617997\n",
    "Raw     - Cross-Validated QWK Score: 0.4675990891579175"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "papermill": {
     "duration": 0.009041,
     "end_time": "2024-09-20T23:29:00.790270",
     "exception": false,
     "start_time": "2024-09-20T23:29:00.781229",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# RESULTS!\n",
    "\n",
    "Cross-Validated QWK Score: 0.47054350562542613"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "papermill": {
     "duration": 0.021183,
     "end_time": "2024-09-20T23:29:00.854965",
     "exception": false,
     "start_time": "2024-09-20T23:29:00.833782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AvgModel:\n",
    "    def __init__(self, models: list[BaseEstimator]):\n",
    "        self.models = models\n",
    "\n",
    "    def predict(self, X: ArrayLike) -> NDArray[np.int_]:\n",
    "        preds: list[NDArray[np.int_]] = []\n",
    "        for model in self.models:\n",
    "            pred = model.predict(X)\n",
    "            preds.append(pred)\n",
    "\n",
    "        return np.mean(preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "papermill": {
     "duration": 0.051348,
     "end_time": "2024-09-20T23:29:00.914906",
     "exception": false,
     "start_time": "2024-09-20T23:29:00.863558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_model = AvgModel(models)\n",
    "test_pred = avg_model.predict(X_test.to_pandas())[:, TARGET_COLS.index(\"PCIAT-PCIAT_Total\")]\n",
    "test_pred_rounded = optimizer.predict(test_pred)\n",
    "test.select(\"id\").with_columns(\n",
    "    pl.Series(\"sii\", pl.Series(\"sii\", test_pred_rounded)),\n",
    ").write_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "papermill": {
     "duration": 0.00833,
     "end_time": "2024-09-20T23:29:00.931995",
     "exception": false,
     "start_time": "2024-09-20T23:29:00.923665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9643020,
     "sourceId": 81933,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 75.110848,
   "end_time": "2024-09-20T23:29:01.764645",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-20T23:27:46.653797",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
