{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "\n",
    "from catboost import CatBoostRegressor, MultiTargetCustomMetric\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from numpy.typing import ArrayLike, NDArray\n",
    "from polars.testing import assert_frame_equal\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedKFold\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"Failed to optimize method\")\n",
    "\n",
    "\n",
    "#DATA_DIR = Path(\"/kaggle/input/child-mind-institute-problematic-internet-use\")\n",
    "DATA_DIR = Path(\"./data\")\n",
    "\n",
    "TARGET_COLS = [\n",
    "    \"PCIAT-PCIAT_01\",\n",
    "    \"PCIAT-PCIAT_02\",\n",
    "    \"PCIAT-PCIAT_03\",\n",
    "    \"PCIAT-PCIAT_04\",\n",
    "    \"PCIAT-PCIAT_05\",\n",
    "    \"PCIAT-PCIAT_06\",\n",
    "    \"PCIAT-PCIAT_07\",\n",
    "    \"PCIAT-PCIAT_08\",\n",
    "    \"PCIAT-PCIAT_09\",\n",
    "    \"PCIAT-PCIAT_10\",\n",
    "    \"PCIAT-PCIAT_11\",\n",
    "    \"PCIAT-PCIAT_12\",\n",
    "    \"PCIAT-PCIAT_13\",\n",
    "    \"PCIAT-PCIAT_14\",\n",
    "    \"PCIAT-PCIAT_15\",\n",
    "    \"PCIAT-PCIAT_16\",\n",
    "    \"PCIAT-PCIAT_17\",\n",
    "    \"PCIAT-PCIAT_18\",\n",
    "    \"PCIAT-PCIAT_19\",\n",
    "    \"PCIAT-PCIAT_20\",\n",
    "    \"PCIAT-PCIAT_Total\",\n",
    "    \"sii\",\n",
    "]\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    \"Basic_Demos-Age\",\n",
    "    \"PreInt_EduHx-computerinternet_hoursday\",\n",
    "    \"SDS-SDS_Total_Raw\",\n",
    "    \"Physical-Height\",\n",
    "    \"FGC-FGC_TL\",\n",
    "    \"Physical-Waist_Circumference\",\n",
    "    \"BIA-BIA_FMI\",\n",
    "    \"BIA-BIA_Fat\",\n",
    "    \"Fitness_Endurance-Time_Sec\",\n",
    "    \"Fitness_Endurance-Max_Stage\",\n",
    "    \"FGC-FGC_CU_Zone\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pl.read_csv(DATA_DIR / \"train.csv\")\n",
    "test = pl.read_csv(DATA_DIR / \"test.csv\")\n",
    "train_test = pl.concat([train, test], how=\"diagonal\")\n",
    "\n",
    "IS_TEST = test.height <= 100\n",
    "\n",
    "assert_frame_equal(train, train_test[: train.height].select(train.columns))\n",
    "assert_frame_equal(test, train_test[train.height :].select(test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast string columns to categorical\n",
    "train_test = train_test.with_columns(cs.string().cast(pl.Categorical).fill_null(\"NAN\"))\n",
    "train = train_test[: train.height]\n",
    "test = train_test[train.height :]\n",
    "\n",
    "# ignore rows with null values in TARGET_COLS\n",
    "train_without_null = train_test.drop_nulls(subset=TARGET_COLS)\n",
    "X = train_without_null.select(FEATURE_COLS)\n",
    "X_test = test.select(FEATURE_COLS)\n",
    "y = train_without_null.select(TARGET_COLS)\n",
    "y_sii = y.get_column(\"sii\").to_numpy()  # ground truth\n",
    "cat_features = X.select(cs.categorical()).columns\n",
    "\n",
    "print(\"Features selected:\")\n",
    "print(cat_features)  # Should be none"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Tubotubo's Quadratic Weighted Kappa metric & Optuna optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTargetQWK(MultiTargetCustomMetric):\n",
    "    def get_final_error(self, error, weight):\n",
    "        return np.sum(error)  # / np.sum(weight)\n",
    "\n",
    "    def is_max_optimal(self):\n",
    "        # if True, the bigger the better\n",
    "        return True\n",
    "\n",
    "    def evaluate(self, approxes, targets, weight):\n",
    "        # approxes: 予測値 (shape: [ターゲット数, サンプル数])\n",
    "        # targets: 実際の値 (shape: [ターゲット数, サンプル数])\n",
    "        # weight: サンプルごとの重み (Noneも可)\n",
    "\n",
    "        approx = np.clip(approxes[-1], 0, 3).round().astype(int)\n",
    "        target = targets[-1]\n",
    "\n",
    "        qwk = cohen_kappa_score(target, approx, weights=\"quadratic\")\n",
    "\n",
    "        return qwk, 1\n",
    "\n",
    "    def get_custom_metric_name(self):\n",
    "        return \"MultiTargetQWK\"\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "from functools import partial\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from typing import List\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "class OptimizedRounder:\n",
    "    def __init__(self, n_classes: int, n_trials: int = 1000):\n",
    "        self.n_classes = n_classes\n",
    "        self.n_trials = n_trials\n",
    "        self.metric = partial(cohen_kappa_score, weights=\"quadratic\")\n",
    "        self.thresholds = None\n",
    "\n",
    "    def fit(self, y_pred: NDArray[np.float_], y_true: NDArray[np.int_]) -> None:\n",
    "        y_pred = self._normalize(y_pred)\n",
    "\n",
    "        def objective(trial: optuna.Trial) -> float:\n",
    "            thresholds = []\n",
    "            for i in range(self.n_classes - 1):\n",
    "                low = thresholds[-1] + 1e-5 if i > 0 else y_pred.min()\n",
    "                high = y_pred.max() - (self.n_classes - i - 2) * 1e-5\n",
    "                th = trial.suggest_float(f\"threshold_{i}\", low, high)\n",
    "                thresholds.append(th)\n",
    "            # Ensure thresholds are strictly increasing\n",
    "            thresholds = np.array(thresholds)\n",
    "            y_pred_rounded = np.digitize(y_pred, thresholds)\n",
    "            kappa = self.metric(y_true, y_pred_rounded)\n",
    "            if np.isnan(kappa) or np.isinf(kappa):\n",
    "                return -1e9  # Return a very low score\n",
    "            return kappa\n",
    "\n",
    "        # Use RandomSampler\n",
    "        sampler = optuna.samplers.RandomSampler()\n",
    "        study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "        study.optimize(\n",
    "            objective,\n",
    "            n_trials=self.n_trials,\n",
    "        )\n",
    "        self.thresholds = sorted([study.best_params[f\"threshold_{i}\"] for i in range(self.n_classes - 1)])\n",
    "\n",
    "    def predict(self, y_pred: NDArray[np.float_]) -> NDArray[np.int_]:\n",
    "        assert self.thresholds is not None, \"fit() must be called before predict()\"\n",
    "        y_pred = self._normalize(y_pred)\n",
    "        thresholds = np.array(self.thresholds)\n",
    "        return np.digitize(y_pred, thresholds)\n",
    "\n",
    "    def _normalize(self, y: NDArray[np.float_]) -> NDArray[np.float_]:\n",
    "        min_pred = y.min()\n",
    "        max_pred = y.max()\n",
    "        if max_pred - min_pred == 0:\n",
    "            return np.zeros_like(y)\n",
    "        return (y - min_pred) / (max_pred - min_pred) * (self.n_classes - 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Start making models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, QuantileTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from catboost import CatBoostRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "\n",
    "# Define your parameters\n",
    "params = dict(\n",
    "    loss_function=\"MultiRMSE\",\n",
    "    eval_metric=\"MultiRMSE\",  # Ensure eval_metric is appropriate\n",
    "    iterations=8000,  # Adjust as needed\n",
    "    learning_rate=0.05,\n",
    "    depth=5,\n",
    "    early_stopping_rounds=50,\n",
    ")\n",
    "\n",
    "# Updated SILLY MODEL with Polynomial Features and Imputation\n",
    "class BaselinePolynomial:\n",
    "    def __init__(self, degree=2, include_bias=False, imputation_strategy='mean'):\n",
    "        \"\"\"\n",
    "        Initializes the SillyManPolynomial model.\n",
    "\n",
    "        Parameters:\n",
    "        - degree (int): Degree of the polynomial features.\n",
    "        - include_bias (bool): Whether to include a bias column.\n",
    "        - imputation_strategy (str): Strategy for imputing missing values ('mean', 'median', 'most_frequent', etc.).\n",
    "        \"\"\"\n",
    "        self.imputer = SimpleImputer(strategy=imputation_strategy)\n",
    "        # self.scaler = QuantileTransformer()\n",
    "        self.poly = PolynomialFeatures(degree=degree, include_bias=include_bias)\n",
    "        self.catboost = CatBoostRegressor(**params)\n",
    "        \n",
    "        \n",
    "    def fit(self, x, y, eval_set, cat_features, verbose=False):\n",
    "        \"\"\"\n",
    "        Fits the model with polynomial feature generation and imputation.\n",
    "\n",
    "        Parameters:\n",
    "        - x (pd.DataFrame): Training features.\n",
    "        - y (pd.Series or pd.DataFrame): Training target.\n",
    "        - eval_set (tuple): Validation set as (X_val, y_val).\n",
    "        - cat_features (list): List of categorical feature names to exclude or handle separately.\n",
    "        - verbose (bool): Verbosity flag.\n",
    "        \"\"\"\n",
    "        self.cat_features = cat_features\n",
    "        \n",
    "        # Separate numerical features (assuming categorical features are to be excluded or handled separately)\n",
    "        if cat_features:\n",
    "            numerical = x.drop(columns=self.cat_features)\n",
    "            categorical = x[self.cat_features]\n",
    "        else:\n",
    "            numerical = x.copy()\n",
    "            categorical = None\n",
    "\n",
    "        # Impute missing values in numerical features\n",
    "        X_train = self.imputer.fit_transform(numerical)\n",
    "        X_val = self.imputer.transform(eval_set[0].drop(columns=self.cat_features)) if self.cat_features else self.imputer.transform(eval_set[0])\n",
    "\n",
    "        # Scale features\n",
    "        # X_train = self.scaler.fit_transform(X_train)\n",
    "        # X_val = self.scaler.transform(X_val)\n",
    "\n",
    "        # Apply Polynomial Feature Generation to numerical features\n",
    "        X_train = self.poly.fit_transform(X_train)\n",
    "        X_val = self.poly.transform(X_val)\n",
    "\n",
    "        # If there are categorical features, append them to the polynomial features\n",
    "        if categorical is not None:\n",
    "            X_train_cat = categorical.values\n",
    "            X_val_cat = eval_set[0][cat_features].values\n",
    "            X_train_processed = np.hstack([X_train, X_train_cat])\n",
    "            X_val_processed = np.hstack([X_val, X_val_cat])\n",
    "            # Update cat_features indices to point to the categorical features appended at the end\n",
    "            new_cat_features = list(range(X_train.shape[1], X_train_processed.shape[1]))\n",
    "        else:\n",
    "            X_train_processed = X_train\n",
    "            X_val_processed = X_val\n",
    "            new_cat_features = []\n",
    "\n",
    "        # Fit CatBoost\n",
    "        self.catboost.fit(\n",
    "            X_train_processed,\n",
    "            y,\n",
    "            eval_set=(X_val_processed, eval_set[1]),\n",
    "            cat_features=new_cat_features,  # Specify categorical features if any\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predicts using the fitted model.\n",
    "\n",
    "        Parameters:\n",
    "        - x (pd.DataFrame): Features for prediction.\n",
    "        - cat_features (list): List of categorical feature names.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: Predictions.\n",
    "        \"\"\"\n",
    "        # Separate numerical features\n",
    "        if cat_features:\n",
    "            numerical = x.drop(columns=self.cat_features)\n",
    "            categorical = x[self.cat_features]\n",
    "        else:\n",
    "            numerical = x.copy()\n",
    "            categorical = None\n",
    "\n",
    "        # Impute missing values\n",
    "        X = self.imputer.transform(numerical)\n",
    "        \n",
    "        # Scale features\n",
    "        # X = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Apply Polynomial Feature Generation\n",
    "        X = self.poly.transform(X)\n",
    "        \n",
    "        # If there are categorical features, append them\n",
    "        if categorical is not None:\n",
    "            X_cat = categorical.values\n",
    "            X_processed = np.hstack([X, X_cat])\n",
    "        else:\n",
    "            X_processed = X\n",
    "\n",
    "        # Predict with each model\n",
    "        cat_pred = self.catboost.predict(X_processed)\n",
    "        \n",
    "        ensemble_pred = cat_pred\n",
    "\n",
    "        return ensemble_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "models: list = []\n",
    "y_pred = np.full((X.height, len(TARGET_COLS)), fill_value=np.nan)\n",
    "\n",
    "#skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=52)\n",
    "rkf = RepeatedKFold(n_splits=5, n_repeats=3, random_state=52)\n",
    "\n",
    "for train_idx, val_idx in rkf.split(X, y_sii):\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Initialize and train model\n",
    "    model = BaselinePolynomial()\n",
    "    #modelcat = CatBoostRegressor(**params)\n",
    "    model.fit(\n",
    "        X_train.to_pandas(),\n",
    "        y_train.to_pandas(),\n",
    "        eval_set=(X_val.to_pandas(), y_val.to_pandas()),\n",
    "        cat_features=cat_features,\n",
    "        verbose=False,\n",
    "    )\n",
    "    models.append(model)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred[val_idx] = model.predict(X_val.to_pandas())\n",
    "\n",
    "# Your existing code\n",
    "optimizer = OptimizedRounder(n_classes=3, n_trials=500)\n",
    "y_pred_total = y_pred[:, TARGET_COLS.index(\"PCIAT-PCIAT_Total\")]\n",
    "\n",
    "# Ensure predictions are valid\n",
    "assert not np.isnan(y_pred_total).any(), \"y_pred_total contains NaN values.\"\n",
    "assert not np.isinf(y_pred_total).any(), \"y_pred_total contains infinite values.\"\n",
    "\n",
    "optimizer.fit(y_pred_total, y_sii)\n",
    "y_pred_rounded = optimizer.predict(y_pred_total)\n",
    "\n",
    "# Calculate QWK\n",
    "qwk = cohen_kappa_score(y_sii, y_pred_rounded, weights=\"quadratic\")\n",
    "print(f\"Cross-Validated QWK Score: {qwk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "OPTIMIZER 'TRIALS' RESULTS:\n",
    "300     - Cross-Validated QWK Score: 0.46576500527929976\n",
    "300     - Cross-Validated QWK Score: 0.46586347333208766\n",
    "400     - Cross-Validated QWK Score: 0.46652503669615863\n",
    "500     - Cross-Validated QWK Score: 0.4691959633503179\n",
    "800     - Cross-Validated QWK Score: 0.4664026888522783\n",
    "\n",
    "OPTIMIZER 'CLASSES' RESULTS:\n",
    "4       - Cross-Validated QWK Score: 0.4691959633503179\n",
    "4       - Cross-Validated QWK Score: 0.4671725712021062\n",
    "5       - Cross-Validated QWK Score: 0.4665550801433156\n",
    "3       - Cross-Validated QWK Score: 0.46461719000259016"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## RESULTS!\n",
    "\n",
    "Baseline SMP    - Cross-Validated QWK Score: 0.4633317017779497\n",
    "RepeatKFold SMP - Cross-Validated QWK Score: 0.4673477280860625\n",
    "\n",
    "Scalers\n",
    "RobustScaler        - Cross-Validated QWK Score: 0.45638992438017634\n",
    "Normalizer          - Cross-Validated QWK Score: 0.43745719311024345\n",
    "StandardScaler      - Cross-Validated QWK Score: 0.44459950754392974\n",
    "QuantileTransformer - Cross-Validated QWK Score: 0.4606737142168206\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgModel:\n",
    "    def __init__(self, models: list[BaseEstimator]):\n",
    "        self.models = models\n",
    "\n",
    "    def predict(self, X: ArrayLike) -> NDArray[np.int_]:\n",
    "        preds: list[NDArray[np.int_]] = []\n",
    "        for model in self.models:\n",
    "            pred = model.predict(X)\n",
    "            preds.append(pred)\n",
    "\n",
    "        return np.mean(preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_model = AvgModel(models)\n",
    "test_pred = avg_model.predict(X_test.to_pandas())[:, TARGET_COLS.index(\"PCIAT-PCIAT_Total\")]\n",
    "test_pred_rounded = optimizer.predict(test_pred)\n",
    "test.select(\"id\").with_columns(\n",
    "    pl.Series(\"sii\", pl.Series(\"sii\", test_pred_rounded)),\n",
    ").write_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
