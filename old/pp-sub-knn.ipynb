{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "papermill": {
     "duration": 3.303708,
     "end_time": "2024-09-20T23:27:53.232262",
     "exception": false,
     "start_time": "2024-09-20T23:27:49.928554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import warnings\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "\n",
    "from catboost import CatBoostRegressor, MultiTargetCustomMetric\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "from numpy.typing import ArrayLike, NDArray\n",
    "from polars.testing import assert_frame_equal\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\n",
    "from colorama import Fore, Style, init\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"Failed to optimize method\")\n",
    "\n",
    "\n",
    "#DATA_DIR = Path(\"/kaggle/input/child-mind-institute-problematic-internet-use\")\n",
    "DATA_DIR = Path(\"./data\")\n",
    "\n",
    "TARGET_COLS = [\n",
    "    \"PCIAT-PCIAT_01\",\n",
    "    \"PCIAT-PCIAT_02\",\n",
    "    \"PCIAT-PCIAT_03\",\n",
    "    \"PCIAT-PCIAT_04\",\n",
    "    \"PCIAT-PCIAT_05\",\n",
    "    \"PCIAT-PCIAT_06\",\n",
    "    \"PCIAT-PCIAT_07\",\n",
    "    \"PCIAT-PCIAT_08\",\n",
    "    \"PCIAT-PCIAT_09\",\n",
    "    \"PCIAT-PCIAT_10\",\n",
    "    \"PCIAT-PCIAT_11\",\n",
    "    \"PCIAT-PCIAT_12\",\n",
    "    \"PCIAT-PCIAT_13\",\n",
    "    \"PCIAT-PCIAT_14\",\n",
    "    \"PCIAT-PCIAT_15\",\n",
    "    \"PCIAT-PCIAT_16\",\n",
    "    \"PCIAT-PCIAT_17\",\n",
    "    \"PCIAT-PCIAT_18\",\n",
    "    \"PCIAT-PCIAT_19\",\n",
    "    \"PCIAT-PCIAT_20\",\n",
    "    \"PCIAT-PCIAT_Total\",\n",
    "    \"sii\",\n",
    "]\n",
    "\n",
    "NORM_FEATURES = [\n",
    "    'Basic_Demos-Age',\n",
    "    'CGAS-CGAS_Score',\n",
    "    'Physical-BMI', 'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "    'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "    \"PreInt_EduHx-computerinternet_hoursday\",\n",
    "]\n",
    "\n",
    "NUM_FEATURES = 51\n",
    "\n",
    "# 51\n",
    "FEATURE_COLS = ['Basic_Demos-Age', 'Basic_Demos-Sex', 'CGAS-CGAS_Score', 'Physical-BMI',\n",
    " 'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    " 'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    " 'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec', 'FGC-FGC_CU',\n",
    " 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND', 'FGC-FGC_GSD', 'FGC-FGC_PU', 'FGC-FGC_SRL',\n",
    " 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n",
    " 'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI', 'BIA-BIA_DEE',\n",
    " 'BIA-BIA_ECW', 'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_ICW',\n",
    " 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM', 'PAQ_C-PAQ_C_Total',\n",
    " 'SDS-SDS_Total_Raw', 'SDS-SDS_Total_T',\n",
    " 'PreInt_EduHx-computerinternet_hoursday', 'X_std', 'X_min', 'X_5', 'Y_std',\n",
    " 'Y_45', 'Z_min', 'anglez_std', 'anglez_min', 'anglez_max', 'light_max',\n",
    " 'battery_voltage_mean', 'battery_voltage_std', 'Fitness_Endurance-Max_Stage']\n",
    "\n",
    "# 70\n",
    "'''FEATURE_COLS = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex', 'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI', 'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference', 'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP', 'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage', 'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec', 'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND', 'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU', 'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR', 'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season', 'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI', 'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM', 'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num', 'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM', 'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season', 'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw', 'SDS-SDS_Total_T', 'PreInt_EduHx-Season', 'PreInt_EduHx-computerinternet_hoursday', 'X_std', 'X_min', 'X_5', 'Y_std', 'Y_45', 'Z_min', 'anglez_std', 'anglez_min', 'anglez_max', 'light_max', 'battery_voltage_mean', 'battery_voltage_std']'''\n",
    "\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    \"Basic_Demos-Age\",\n",
    "    \"PreInt_EduHx-computerinternet_hoursday\",\n",
    "    \"SDS-SDS_Total_Raw\",\n",
    "    \"Physical-Height\",\n",
    "    \"FGC-FGC_TL\",\n",
    "    \"Physical-Waist_Circumference\",\n",
    "    \"BIA-BIA_FMI\",\n",
    "    \"BIA-BIA_Fat\",\n",
    "    \"Fitness_Endurance-Time_Sec\",\n",
    "    \"Fitness_Endurance-Max_Stage\",\n",
    "    \"FGC-FGC_CU_Zone\",\n",
    "]\n",
    "\n",
    "# Sheikh's\n",
    "'''FEATURE_COLS = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "       'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n",
    "       'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "       'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "       'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n",
    "       'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "       'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "       'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "       'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "       'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n",
    "       'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "       'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "       'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "       'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "       'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n",
    "       'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n",
    "       'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n",
    "       'PreInt_EduHx-computerinternet_hoursday']'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit to Ravi Ramakrishnan for his great Preprocessing class\n",
    "# https://www.kaggle.com/code/ravi20076/cmi2024-baseline-v2\n",
    "\n",
    "# Color printing\n",
    "def PrintColor(text: str, color = Fore.BLUE, style = Style.BRIGHT):\n",
    "    \"Prints color outputs using colorama using a text F-string\"\n",
    "    print(style + color + text + Style.RESET_ALL)\n",
    "\n",
    "class Preprocessor:\n",
    "    \"This class organizes the preprocessing steps for the train-test data into a single code block\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, cat_imp_val : str= \"missing\", ip_path: str = DATA_DIR\n",
    "    ):\n",
    "        self.cat_imp_val = cat_imp_val\n",
    "        self.ip_path     = ip_path\n",
    "        \n",
    "    def make_pqfile_cols(\n",
    "        self, verbose: bool = False, label: str = \"Train\"\n",
    "    )->pl.DataFrame:\n",
    "        \"This method collates the id level parquet files and creates the aggregation columns in a polars dataframe\"\n",
    "\n",
    "        cols = [\"X\", \"Y\", \"Z\", \"enmo\", \"anglez\", \"light\", \"battery_voltage\"]\n",
    "        \n",
    "        ip_path   = os.path.join(self.ip_path, f\"series_{label.lower()}.parquet\")\n",
    "        all_files = os.listdir(ip_path)\n",
    "\n",
    "        for file_nb, file in tqdm(enumerate(all_files)):\n",
    "            df = \\\n",
    "            pl.scan_parquet(\n",
    "                os.path.join(ip_path, file, f\"part-0.parquet\")\n",
    "            ).select(pl.col(cols)).\\\n",
    "            collect().\\\n",
    "            describe(\n",
    "                percentiles = np.arange(0.05, 0.95, 0.10)\n",
    "            ).\\\n",
    "            filter(~pl.col(\"statistic\").is_in([\"count\", \"null_count\"])).\\\n",
    "            unpivot(index = \"statistic\").\\\n",
    "            with_columns(\n",
    "                pl.concat_str([pl.col(\"variable\"), pl.col(\"statistic\")],separator = \"_\",).alias(\"myvar\")\n",
    "            ).\\\n",
    "            with_columns(pl.col(\"myvar\").str.replace(r\"\\%\", \"\")).\\\n",
    "            select([\"myvar\", \"value\"]).\\\n",
    "            transpose(column_names = \"myvar\").\\\n",
    "            select(pl.all().shrink_dtype()).\\\n",
    "            with_columns(\n",
    "                pl.Series(\"id\", np.array(re.sub(\"id=\", \"\", file)))\n",
    "            )\n",
    "\n",
    "            if file_nb == 0:\n",
    "                op_df = df.clone()\n",
    "            elif file_nb > 0:\n",
    "                op_df = pl.concat([op_df, df], how = \"vertical_relaxed\")\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"---> Shapes = {op_df.shape}\")\n",
    "                else:\n",
    "                    pass\n",
    "            del df\n",
    "\n",
    "        PrintColor(f\"---> {label} - shape = {op_df.shape}\", color = Fore.CYAN)\n",
    "        return op_df\n",
    "\n",
    "    def pp_data(\n",
    "        self, df: pl.DataFrame, label: str = \"Train\", cat_cols: list = [], \n",
    "    ):\n",
    "        \"This method preprocesses the train-test data with requisite steps\"\n",
    "        \n",
    "        PrintColor(f\"\\n --- Data Processing - {label} --- \\n\")\n",
    "        PrintColor(f\"---> Shape = {df.shape} - memory usage {df.estimated_size('mb') :.3f} MB\", \n",
    "                   color = Fore.CYAN\n",
    "                  )\n",
    "        \n",
    "        if label == \"Train\":\n",
    "            cat_cols = df.select(cs.string().exclude(\"id\")).columns\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        df    = df.with_columns(pl.col(cat_cols).fill_null(self.cat_imp_val).cast(pl.Categorical))\n",
    "        op_df = self.make_pqfile_cols(label = label)\n",
    "        df    = df.join(op_df, how = \"left\", on = \"id\")\n",
    "        df    = df.select(pl.all().shrink_dtype())\n",
    "        del op_df\n",
    "        \n",
    "        PrintColor(f\"---> Shape = {df.shape} - memory usage {df.estimated_size('mb') :.3f} MB\", \n",
    "                   color = Fore.CYAN\n",
    "                  )\n",
    "        return df, cat_cols\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "papermill": {
     "duration": 0.202543,
     "end_time": "2024-09-20T23:27:53.443436",
     "exception": false,
     "start_time": "2024-09-20T23:27:53.240893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pl.read_csv(DATA_DIR / \"train.csv\").drop(['PCIAT-Season',], strict=False)#.drop(['Basic_Demos-Enroll_Season', 'CGAS-Season'], strict=False)\n",
    "test = pl.read_csv(DATA_DIR / \"test.csv\").drop(['PCIAT-Season'], strict=False)#.drop(['Basic_Demos-Enroll_Season', 'CGAS-Season'], strict=False)\n",
    "\n",
    "# Ensure 'id' is a string in both DataFrames\n",
    "train = train.with_columns(pl.col('id').cast(pl.Utf8))\n",
    "test = test.with_columns(pl.col('id').cast(pl.Utf8))\n",
    "\n",
    "pp = Preprocessor()\n",
    "\n",
    "# Use StringCache to handle categoricals consistently\n",
    "with pl.StringCache():\n",
    "    train, cat_cols = pp.pp_data(train, \"Train\")\n",
    "    test, _ = pp.pp_data(test, \"Test\", cat_cols)\n",
    "\n",
    "# Ensure data types match between train and test\n",
    "for col in train.columns:\n",
    "    if col in test.columns:\n",
    "        train_dtype = train.schema[col]\n",
    "        test_dtype = test.schema[col]\n",
    "        if train_dtype != test_dtype:\n",
    "            # Cast test column to train's data type\n",
    "            test = test.with_columns(pl.col(col).cast(train_dtype))\n",
    "    else:\n",
    "        # Column in train but not in test, fill with nulls in test\n",
    "        test = test.with_columns(pl.lit(None).cast(train.schema[col]).alias(col))\n",
    "\n",
    "# Reorder test columns to match train\n",
    "test = test.select(train.columns)\n",
    "\n",
    "# Concatenate train and test\n",
    "train_test = pl.concat([train, test], how=\"vertical\")\n",
    "\n",
    "IS_TEST = test.height <= 100\n",
    "\n",
    "assert_frame_equal(train, train_test[: train.height].select(train.columns))\n",
    "assert_frame_equal(test, train_test[train.height :].select(test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "papermill": {
     "duration": 0.066498,
     "end_time": "2024-09-20T23:27:53.518169",
     "exception": false,
     "start_time": "2024-09-20T23:27:53.451671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#FEATURE_COLS = train.drop(TARGET_COLS, strict=False).drop([\"id\"], strict=False).columns\n",
    "\n",
    "# Cast string columns to categorical\n",
    "train_test = train_test.with_columns(cs.string().cast(pl.Categorical).fill_null(\"NAN\"))\n",
    "train = train_test[: train.height]\n",
    "test = train_test[train.height :]\n",
    "\n",
    "# ignore rows with null values in TARGET_COLS\n",
    "train_without_null = train_test.drop_nulls(subset=TARGET_COLS)\n",
    "X = train_without_null.select(FEATURE_COLS)\n",
    "X_test = test.select(FEATURE_COLS)\n",
    "y = train_without_null.select(TARGET_COLS)\n",
    "y_sii = y.get_column(\"sii\").to_numpy()  # ground truth\n",
    "cat_features = X.select(cs.categorical()).columns\n",
    "\n",
    "print(\"Features selected:\")\n",
    "print(cat_features)  # Should be none"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Feature Selection!\n",
    "Last ran 09/22/24\n",
    "\n",
    "    FEATURE_COLS = ['Basic_Demos-Age', 'Basic_Demos-Sex', 'CGAS-CGAS_Score', 'Physical-BMI',\n",
    "     'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "     'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "     'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec', 'FGC-FGC_CU',\n",
    "     'FGC-FGC_CU_Zone', 'FGC-FGC_GSND', 'FGC-FGC_GSD', 'FGC-FGC_PU', 'FGC-FGC_SRL',\n",
    "     'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n",
    "     'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI', 'BIA-BIA_DEE',\n",
    "     'BIA-BIA_ECW', 'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_ICW',\n",
    "     'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM', 'PAQ_C-PAQ_C_Total',\n",
    "     'SDS-SDS_Total_Raw', 'SDS-SDS_Total_T',\n",
    "     'PreInt_EduHx-computerinternet_hoursday', 'X_std', 'X_min', 'X_5', 'Y_std',\n",
    "     'Y_45', 'Z_min', 'anglez_std', 'anglez_min', 'anglez_max', 'light_max',\n",
    "     'battery_voltage_mean', 'battery_voltage_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "NUM_FEATURES_RANKED = 50\n",
    "RUN_FEATURE_RANKING = False\n",
    "\n",
    "if RUN_FEATURE_RANKING:\n",
    "    selector_params = dict(\n",
    "        loss_function=\"MultiRMSE\",\n",
    "        eval_metric=\"MultiRMSE\",\n",
    "        n_estimators=8000,  # Adjust as needed\n",
    "        learning_rate=0.05,\n",
    "        depth=5,\n",
    "        early_stopping_rounds=50,\n",
    "        bagging_temperature=15,\n",
    "        l2_leaf_reg=5\n",
    "    )\n",
    "    selector = RFE(estimator=CatBoostRegressor(**selector_params), n_features_to_select=NUM_FEATURES, step=1)\n",
    "    \n",
    "    numerical = X.drop(cat_features)\n",
    "    \n",
    "    selector.fit_transform(numerical, y)\n",
    "    \n",
    "    FEATURE_COLS = selector.get_feature_names_out()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PrintColor(f\"--- Feature Selection - {len(FEATURE_COLS)} Chosen ---\", color = Fore.BLUE, style = Style.BRIGHT)\n",
    "print(FEATURE_COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "papermill": {
     "duration": 0.008025,
     "end_time": "2024-09-20T23:27:53.534860",
     "exception": false,
     "start_time": "2024-09-20T23:27:53.526835",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tubotubo's Quadratic Weighted Kappa metric & Optuna optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "papermill": {
     "duration": 0.03706,
     "end_time": "2024-09-20T23:27:53.580139",
     "exception": false,
     "start_time": "2024-09-20T23:27:53.543079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiTargetQWK(MultiTargetCustomMetric):\n",
    "    def get_final_error(self, error, weight):\n",
    "        return np.sum(error)  # / np.sum(weight)\n",
    "\n",
    "    def is_max_optimal(self):\n",
    "        # if True, the bigger the better\n",
    "        return True\n",
    "\n",
    "    def evaluate(self, approxes, targets, weight):\n",
    "        # approxes: 予測値 (shape: [ターゲット数, サンプル数])\n",
    "        # targets: 実際の値 (shape: [ターゲット数, サンプル数])\n",
    "        # weight: サンプルごとの重み (Noneも可)\n",
    "\n",
    "        approx = np.clip(approxes[-1], 0, 3).round().astype(int)\n",
    "        target = targets[-1]\n",
    "\n",
    "        qwk = cohen_kappa_score(target, approx, weights=\"quadratic\")\n",
    "\n",
    "        return qwk, 1\n",
    "\n",
    "    def get_custom_metric_name(self):\n",
    "        return \"MultiTargetQWK\"\n",
    "\n",
    "\n",
    "class OptimizedRounder:\n",
    "    \"\"\"\n",
    "    A class for optimizing the rounding of continuous predictions into discrete class labels using Optuna.\n",
    "    The optimization process maximizes the Quadratic Weighted Kappa score by learning thresholds that separate\n",
    "    continuous predictions into class intervals.\n",
    "\n",
    "    Args:\n",
    "        n_classes (int): The number of discrete class labels.\n",
    "        n_trials (int, optional): The number of trials for the Optuna optimization. Defaults to 100.\n",
    "\n",
    "    Attributes:\n",
    "        n_classes (int): The number of discrete class labels.\n",
    "        labels (NDArray[np.int_]): An array of class labels from 0 to `n_classes - 1`.\n",
    "        n_trials (int): The number of optimization trials.\n",
    "        metric (Callable): The Quadratic Weighted Kappa score metric used for optimization.\n",
    "        thresholds (List[float]): The optimized thresholds learned after calling `fit()`.\n",
    "\n",
    "    Methods:\n",
    "        fit(y_pred: NDArray[np.float_], y_true: NDArray[np.int_]) -> None:\n",
    "            Fits the rounding thresholds based on continuous predictions and ground truth labels.\n",
    "\n",
    "            Args:\n",
    "                y_pred (NDArray[np.float_]): Continuous predictions that need to be rounded.\n",
    "                y_true (NDArray[np.int_]): Ground truth class labels.\n",
    "\n",
    "            Returns:\n",
    "                None\n",
    "\n",
    "        predict(y_pred: NDArray[np.float_]) -> NDArray[np.int_]:\n",
    "            Predicts discrete class labels by rounding continuous predictions using the fitted thresholds.\n",
    "            `fit()` must be called before `predict()`.\n",
    "\n",
    "            Args:\n",
    "                y_pred (NDArray[np.float_]): Continuous predictions to be rounded.\n",
    "\n",
    "            Returns:\n",
    "                NDArray[np.int_]: Predicted class labels.\n",
    "\n",
    "        _normalize(y: NDArray[np.float_]) -> NDArray[np.float_]:\n",
    "            Normalizes the continuous values to the range [0, `n_classes - 1`].\n",
    "\n",
    "            Args:\n",
    "                y (NDArray[np.float_]): Continuous values to be normalized.\n",
    "\n",
    "            Returns:\n",
    "                NDArray[np.float_]: Normalized values.\n",
    "\n",
    "    References:\n",
    "        - This implementation uses Optuna for threshold optimization.\n",
    "        - Quadratic Weighted Kappa is used as the evaluation metric.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_classes: int, n_trials: int = 100):\n",
    "        self.n_classes = n_classes\n",
    "        self.labels = np.arange(n_classes)\n",
    "        self.n_trials = n_trials\n",
    "        self.metric = partial(cohen_kappa_score, weights=\"quadratic\")\n",
    "\n",
    "    def fit(self, y_pred: NDArray[np.float_], y_true: NDArray[np.int_]) -> None:\n",
    "        y_pred = self._normalize(y_pred)\n",
    "\n",
    "        def objective(trial: optuna.Trial) -> float:\n",
    "            thresholds = []\n",
    "            for i in range(self.n_classes - 1):\n",
    "                low = max(thresholds) if i > 0 else min(self.labels)\n",
    "                high = max(self.labels)\n",
    "                th = trial.suggest_float(f\"threshold_{i}\", low, high)\n",
    "                thresholds.append(th)\n",
    "            try:\n",
    "                y_pred_rounded = np.digitize(y_pred, thresholds)\n",
    "            except ValueError:\n",
    "                return -100\n",
    "            return self.metric(y_true, y_pred_rounded)\n",
    "\n",
    "        optuna.logging.disable_default_handler()\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(\n",
    "            objective,\n",
    "            n_trials=self.n_trials,\n",
    "        )\n",
    "        self.thresholds = [study.best_params[f\"threshold_{i}\"] for i in range(self.n_classes - 1)]\n",
    "\n",
    "    def predict(self, y_pred: NDArray[np.float_]) -> NDArray[np.int_]:\n",
    "        assert hasattr(self, \"thresholds\"), \"fit() must be called before predict()\"\n",
    "        y_pred = self._normalize(y_pred)\n",
    "        return np.digitize(y_pred, self.thresholds)\n",
    "\n",
    "    def _normalize(self, y: NDArray[np.float_]) -> NDArray[np.float_]:\n",
    "        # normalize y_pred to [0, n_classes - 1]\n",
    "        return (y - y.min()) / (y.max() - y.min()) * (self.n_classes - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "papermill": {
     "duration": 0.008373,
     "end_time": "2024-09-20T23:27:53.597003",
     "exception": false,
     "start_time": "2024-09-20T23:27:53.588630",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Start making models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "papermill": {
     "duration": 0.08071,
     "end_time": "2024-09-20T23:27:53.899286",
     "exception": false,
     "start_time": "2024-09-20T23:27:53.818576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "params = dict(\n",
    "    loss_function=\"MultiRMSE\",\n",
    "    eval_metric=\"MultiRMSE\",  # Ensure eval_metric is appropriate\n",
    "    iterations=7000,  # Adjust as needed\n",
    "    learning_rate=0.05,\n",
    "    depth=5,\n",
    "    early_stopping_rounds=50,\n",
    ")\n",
    "\n",
    "# Updated SILLY MODEL with RFE\n",
    "class SillyManRFE():\n",
    "    def __init__(self, n_features_to_select=10):\n",
    "        self.selector = RFE(estimator=CatBoostRegressor(**params), n_features_to_select=n_features_to_select, step=1)\n",
    "        self.cat = CatBoostRegressor(**params)\n",
    "        \n",
    "    def fit(self, x, y, eval_set, cat_features, verbose=False):\n",
    "        self.cat_features = cat_features\n",
    "        \n",
    "        # Apply RFE for feature selection\n",
    "        X_selected = self.selector.fit_transform(x, y)\n",
    "        \n",
    "        # Identify selected feature indices\n",
    "        selected_features = np.array(x.columns)[self.selector.support_]\n",
    "        \n",
    "        # Identify categorical features after selection\n",
    "        new_cat_features = [i for i, col in enumerate(x.columns) if col in cat_features and col in selected_features]\n",
    "        \n",
    "        # Prepare evaluation set\n",
    "        X_val_selected = self.selector.transform(eval_set[0])\n",
    "        \n",
    "        # Fit CatBoost\n",
    "        self.cat.fit(\n",
    "            X_selected,\n",
    "            y,\n",
    "            eval_set=(X_val_selected, eval_set[1]),\n",
    "            cat_features=new_cat_features,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "    def predict(self, x):\n",
    "        X_selected = self.selector.transform(x)\n",
    "        return self.cat.predict(X_selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import (\n",
    "    PolynomialFeatures,\n",
    "    SplineTransformer,\n",
    "    StandardScaler,\n",
    "    MinMaxScaler,\n",
    "    QuantileTransformer\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import umap\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "class SillyManCombinational:\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_catboost_params=None,\n",
    "        base_xgboost_params=None,\n",
    "        spline_catboost_params=None,\n",
    "        spline_xgboost_params=None,\n",
    "        spline_lgbm_params=None,\n",
    "        include_bias=False,\n",
    "        imputation_strategy='mean',\n",
    "        use_spline=True,\n",
    "        use_poly=False,\n",
    "        use_raw=False,\n",
    "        use_standard_scaler=False,\n",
    "        use_minmax_scaler=False,\n",
    "        use_quantile_transformer=False,\n",
    "        base_estimators=['catboost'],  # options: ['catboost', 'xgboost', 'raw']\n",
    "        final_estimators=['catboost'],  # options: ['catboost', 'xgboost']\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the SillyManCombinational model with options to include various transformers and base estimators.\n",
    "\n",
    "        Parameters:\n",
    "        - include_bias (bool): Whether to include a bias column.\n",
    "        - imputation_strategy (str): Strategy for imputing missing values.\n",
    "        - use_spline (bool): Whether to include Spline transformed features.\n",
    "        - use_poly (bool): Whether to include Polynomial transformed features.\n",
    "        - use_raw (bool): Whether to include raw numerical features.\n",
    "        - use_standard_scaler (bool): Whether to include StandardScaler transformed features.\n",
    "        - use_minmax_scaler (bool): Whether to include MinMaxScaler transformed features.\n",
    "        - use_quantile_transformer (bool): Whether to include QuantileTransformer transformed features.\n",
    "        - base_estimators (list): List of estimators to use for base models ['catboost', 'xgboost', 'raw'].\n",
    "        - final_estimators (list): List of estimators to use for final model ['catboost', 'xgboost'].\n",
    "        \"\"\"\n",
    "        self.base_catboost_params=base_catboost_params\n",
    "        self.base_xgboost_params=base_xgboost_params\n",
    "        self.spline_catboost_params=spline_catboost_params\n",
    "        self.spline_xgboost_params=spline_xgboost_params\n",
    "        self.spline_lgbm_params=spline_lgbm_params\n",
    "        \n",
    "        # Transformer toggles\n",
    "        self.use_spline = use_spline\n",
    "        self.use_poly = use_poly\n",
    "        self.use_raw_transformer = use_raw  # Renamed to avoid confusion with 'raw' in base_estimators\n",
    "        self.use_standard_scaler = use_standard_scaler\n",
    "        self.use_minmax_scaler = use_minmax_scaler\n",
    "        self.use_quantile_transformer = use_quantile_transformer\n",
    "\n",
    "        # Estimator types\n",
    "        self.base_estimators_types = base_estimators\n",
    "        self.final_estimators_types = final_estimators\n",
    "\n",
    "        # Validation: If 'raw' is in base_estimators, 'use_raw_transformer' must be True\n",
    "        #if 'raw' in self.base_estimators_types and not self.use_raw_transformer:\n",
    "        #    raise ValueError(\"Setting 'raw' in base_estimators requires 'use_raw=True'.\")\n",
    "\n",
    "        self.imputer = SimpleImputer(strategy=imputation_strategy)\n",
    "        if self.use_poly:\n",
    "            self.poly = PolynomialFeatures(degree=2, include_bias=include_bias)\n",
    "        if self.use_spline:\n",
    "            self.spline = SplineTransformer(n_knots=5, degree=3, include_bias=include_bias)\n",
    "        if self.use_standard_scaler:\n",
    "            self.standard_scaler = StandardScaler()\n",
    "        if self.use_minmax_scaler:\n",
    "            self.minmax_scaler = MinMaxScaler()\n",
    "        if self.use_quantile_transformer:\n",
    "            self.quantile_transformer = QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "        # Base estimators dict\n",
    "        self.base_estimators = {}\n",
    "        # Final estimator(s)\n",
    "        self.final_estimators = {}\n",
    "        # To store categorical feature indices in final estimators\n",
    "        self.final_cat_features = {}\n",
    "\n",
    "    def fit(self, x, y, eval_set, cat_features, verbose=False):\n",
    "        \"\"\"\n",
    "        Fits the model with selected feature transformations and imputation.\n",
    "\n",
    "        Parameters:\n",
    "        - x (pd.DataFrame): Training features.\n",
    "        - y (pd.Series or pd.DataFrame): Training target.\n",
    "        - eval_set (tuple): Validation set as (X_val, y_val).\n",
    "        - cat_features (list): List of categorical feature names.\n",
    "        - verbose (bool): Verbosity flag.\n",
    "        \"\"\"\n",
    "        self.cat_features = cat_features\n",
    "\n",
    "        # Separate numerical and categorical features\n",
    "        if cat_features:\n",
    "            numerical = x.drop(columns=self.cat_features)\n",
    "            categorical = x[self.cat_features]\n",
    "        else:\n",
    "            numerical = x.copy()\n",
    "            categorical = None\n",
    "\n",
    "        # Impute missing values in numerical features\n",
    "        X_train_imputed = self.imputer.fit_transform(numerical)\n",
    "        X_val_imputed = self.imputer.transform(\n",
    "            eval_set[0].drop(columns=self.cat_features) if self.cat_features else eval_set[0]\n",
    "        )\n",
    "        \n",
    "        # Step 7: Apply KMeans clustering\n",
    "        n_clusters = 4  # Number of clusters based on unique values in 'sii'\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        clusters = kmeans.fit_predict(X_train_imputed)\n",
    "        \n",
    "        # Step 1: Initialize UMAP with 3 components\n",
    "        umap_3d = umap.UMAP(n_components=3)\n",
    "        \n",
    "        # Step 2: Fit and transform the encoded dataset (df_encoded)\n",
    "        df_umap_3d = umap_3d.fit_transform(X_train_imputed)\n",
    "        \n",
    "        # Step 3: Create a DataFrame for the UMAP results and include cluster labels and sii column\n",
    "        df_umap_plot = pd.DataFrame(df_umap_3d, columns=['Dim1', 'Dim2', 'Dim3'])\n",
    "        df_umap_plot['Cluster'] = clusters\n",
    "        df_umap_plot['sii'] = y_train['sii']\n",
    "        \n",
    "        # Step 4: Plot the 3D scatter plot\n",
    "        fig = px.scatter_3d(df_umap_plot, x='Dim1', y='Dim2', z='Dim3', color='sii',\n",
    "                            title=\"3D UMAP Visualization of Clusters\",\n",
    "                            labels={'Dim1': 'Component 1', 'Dim2': 'Component 2', 'Dim3': 'Component 3'},\n",
    "                            color_discrete_sequence=px.colors.qualitative.Plotly)\n",
    "        \n",
    "        # Step 5: Show the plot\n",
    "        fig.show()\n",
    "\n",
    "        # Transformer functions mapping\n",
    "        transformers = {}\n",
    "        if self.use_spline:\n",
    "            transformers['spline'] = self.spline\n",
    "        if self.use_poly:\n",
    "            transformers['poly'] = self.poly\n",
    "        if self.use_raw_transformer:\n",
    "            transformers['raw_transformer'] = None  # raw data, no transformer\n",
    "        if self.use_standard_scaler:\n",
    "            transformers['standard_scaler'] = self.standard_scaler\n",
    "        if self.use_minmax_scaler:\n",
    "            transformers['minmax_scaler'] = self.minmax_scaler\n",
    "        if self.use_quantile_transformer:\n",
    "            transformers['quantile_transformer'] = self.quantile_transformer\n",
    "\n",
    "        if not transformers:\n",
    "            raise ValueError(\"At least one transformer must be selected.\")\n",
    "\n",
    "        transformed_train_features = []\n",
    "        transformed_val_features = []\n",
    "        transformed_feature_names = []\n",
    "\n",
    "        # Apply each transformer and store transformed features\n",
    "        for name, transformer in transformers.items():\n",
    "            if transformer is not None:\n",
    "                # Fit and transform\n",
    "                X_train_transformed = transformer.fit_transform(X_train_imputed)\n",
    "                X_val_transformed = transformer.transform(X_val_imputed)\n",
    "                feature_names = transformer.get_feature_names_out(input_features=numerical.columns)\n",
    "                feature_names = [f'{name}_{feat}' for feat in feature_names]\n",
    "            else:\n",
    "                # Raw data (no transformation)\n",
    "                X_train_transformed = X_train_imputed\n",
    "                X_val_transformed = X_val_imputed\n",
    "                feature_names = [f'raw_{feat}' for feat in numerical.columns]\n",
    "\n",
    "            # Create DataFrames\n",
    "            X_train_df = pd.DataFrame(X_train_transformed, columns=feature_names)\n",
    "            X_val_df = pd.DataFrame(X_val_transformed, columns=feature_names)\n",
    "\n",
    "            # Store transformed features\n",
    "            transformed_train_features.append(X_train_df)\n",
    "            transformed_val_features.append(X_val_df)\n",
    "            transformed_feature_names.append(name)\n",
    "\n",
    "        # Handle categorical features\n",
    "        if categorical is not None:\n",
    "            X_train_cat = categorical.reset_index(drop=True)\n",
    "            X_val_cat = eval_set[0][self.cat_features].reset_index(drop=True)\n",
    "        else:\n",
    "            X_train_cat = None\n",
    "            X_val_cat = None\n",
    "\n",
    "        # Initialize lists to collect base estimator predictions\n",
    "        base_train_preds = []\n",
    "        base_val_preds = []\n",
    "\n",
    "        # If base_estimators are specified and not empty\n",
    "        if self.base_estimators_types and self.base_estimators_types != ['']:\n",
    "            for idx, name in enumerate(transformed_feature_names):\n",
    "                X_train_df = transformed_train_features[idx]\n",
    "                X_val_df = transformed_val_features[idx]\n",
    "\n",
    "                # Append categorical features if any\n",
    "                if X_train_cat is not None:\n",
    "                    X_train_df = pd.concat([X_train_df.reset_index(drop=True), X_train_cat], axis=1)\n",
    "                    X_val_df = pd.concat([X_val_df.reset_index(drop=True), X_val_cat], axis=1)\n",
    "                    # Update cat_features indices to point to the categorical features appended at the end\n",
    "                    new_cat_features = list(range(X_train_df.shape[1] - len(self.cat_features), X_train_df.shape[1]))\n",
    "                else:\n",
    "                    new_cat_features = []\n",
    "\n",
    "                for est_type in self.base_estimators_types:\n",
    "                    if est_type == 'raw':\n",
    "                        # Include transformed features directly as meta-features\n",
    "                        base_train_preds.append(X_train_df.copy())\n",
    "                        base_val_preds.append(X_val_df.copy())\n",
    "                        continue  # Skip training any estimator\n",
    "                    est_name = f'{name}_{est_type}'\n",
    "                    if est_type == 'catboost':\n",
    "                        estimator = CatBoostRegressor(**self.base_catboost_params)\n",
    "                        estimator.fit(\n",
    "                            X_train_df,\n",
    "                            y,\n",
    "                            eval_set=(X_val_df, eval_set[1]),\n",
    "                            cat_features=new_cat_features,\n",
    "                            verbose=verbose\n",
    "                        )\n",
    "                    elif est_type == 'xgboost':\n",
    "                        estimator = XGBRegressor(**self.base_xgboost_params)\n",
    "                        estimator.fit(\n",
    "                            X_train_df,\n",
    "                            y,\n",
    "                            eval_set=[(X_val_df, eval_set[1])],\n",
    "                            verbose=verbose\n",
    "                        )\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unsupported estimator type: {est_type}\")\n",
    "\n",
    "                    # Save estimator\n",
    "                    self.base_estimators[est_name] = estimator\n",
    "\n",
    "                    # Collect predictions\n",
    "                    train_pred = estimator.predict(X_train_df)\n",
    "                    val_pred = estimator.predict(X_val_df)\n",
    "\n",
    "                    # Ensure predictions are 2D arrays\n",
    "                    if train_pred.ndim == 1:\n",
    "                        train_pred = train_pred.reshape(-1, 1)\n",
    "                        val_pred = val_pred.reshape(-1, 1)\n",
    "\n",
    "                    # Generate appropriate column names for multi-output\n",
    "                    n_outputs = train_pred.shape[1] if train_pred.ndim > 1 else 1\n",
    "                    if n_outputs == 1:\n",
    "                        columns = [est_name]\n",
    "                    else:\n",
    "                        columns = [f'{est_name}_output_{i}' for i in range(n_outputs)]\n",
    "\n",
    "                    # Create DataFrames with correct column names\n",
    "                    base_train_preds.append(pd.DataFrame(train_pred, columns=columns))\n",
    "                    base_val_preds.append(pd.DataFrame(val_pred, columns=columns))\n",
    "\n",
    "            # Combine base estimator predictions\n",
    "            if base_train_preds and base_val_preds:\n",
    "                X_train_meta = pd.concat(base_train_preds, axis=1).reset_index(drop=True)\n",
    "                X_val_meta = pd.concat(base_val_preds, axis=1).reset_index(drop=True)\n",
    "            else:\n",
    "                raise ValueError(\"Base estimators did not produce any predictions.\")\n",
    "        else:\n",
    "            # If no base estimators, use transformed features directly\n",
    "            X_train_meta = pd.concat(transformed_train_features, axis=1).reset_index(drop=True)\n",
    "            X_val_meta = pd.concat(transformed_val_features, axis=1).reset_index(drop=True)\n",
    "\n",
    "            # Append categorical features if any\n",
    "            if X_train_cat is not None:\n",
    "                X_train_meta = pd.concat([X_train_meta.reset_index(drop=True), X_train_cat], axis=1)\n",
    "                X_val_meta = pd.concat([X_val_meta.reset_index(drop=True), X_val_cat], axis=1)\n",
    "                # Update cat_features indices to point to the categorical features appended at the end\n",
    "                new_cat_features = list(range(X_train_meta.shape[1] - len(self.cat_features), X_train_meta.shape[1]))\n",
    "            else:\n",
    "                new_cat_features = []\n",
    "\n",
    "        # Train final estimator(s)\n",
    "        for est_type in self.final_estimators_types:\n",
    "            est_name = f'final_{est_type}'\n",
    "            if est_type == 'catboost':\n",
    "                final_estimator = CatBoostRegressor(**self.spline_catboost_params)\n",
    "                final_estimator.fit(\n",
    "                    X_train_meta,\n",
    "                    y,\n",
    "                    eval_set=(X_val_meta, eval_set[1]),\n",
    "                    cat_features=new_cat_features,\n",
    "                    verbose=verbose\n",
    "                )\n",
    "            elif est_type == 'xgboost':\n",
    "                final_estimator = XGBRegressor(**self.spline_xgboost_params)\n",
    "                final_estimator.fit(\n",
    "                    X_train_meta,\n",
    "                    y,\n",
    "                    eval_set=[(X_val_meta, eval_set[1])],\n",
    "                    verbose=verbose\n",
    "                )\n",
    "            elif est_type == 'lgbm':\n",
    "                final_estimator = lgb.LGBMRegressor(**self.spline_lgbm_params)\n",
    "                final_estimator.fit(\n",
    "                    X_train_meta,\n",
    "                    y['sii'],\n",
    "                    eval_set=[(X_val_meta, eval_set[1]['sii'])],\n",
    "                )\n",
    "                \n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported final estimator type: {est_type}\")\n",
    "            # Save final estimator\n",
    "            self.final_estimators[est_type] = final_estimator\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predicts using the fitted model.\n",
    "\n",
    "        Parameters:\n",
    "        - x (pd.DataFrame): Features for prediction.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: Predictions.\n",
    "        \"\"\"\n",
    "        # Separate numerical and categorical features\n",
    "        if self.cat_features:\n",
    "            numerical = x.drop(columns=self.cat_features)\n",
    "            categorical = x[self.cat_features]\n",
    "        else:\n",
    "            numerical = x.copy()\n",
    "            categorical = None\n",
    "\n",
    "        # Impute missing values\n",
    "        X_imputed = self.imputer.transform(numerical)\n",
    "        \n",
    "        # Transformer functions mapping (same as in fit)\n",
    "        transformers = {}\n",
    "        if self.use_spline:\n",
    "            transformers['spline'] = self.spline\n",
    "        if self.use_poly:\n",
    "            transformers['poly'] = self.poly\n",
    "        if self.use_raw_transformer:\n",
    "            transformers['raw_transformer'] = None  # raw data, no transformer\n",
    "        if self.use_standard_scaler:\n",
    "            transformers['standard_scaler'] = self.standard_scaler\n",
    "        if self.use_minmax_scaler:\n",
    "            transformers['minmax_scaler'] = self.minmax_scaler\n",
    "        if self.use_quantile_transformer:\n",
    "            transformers['quantile_transformer'] = self.quantile_transformer\n",
    "\n",
    "        transformed_features = []\n",
    "        transformed_feature_names = []\n",
    "\n",
    "        # Apply each transformer and store transformed features\n",
    "        for name, transformer in transformers.items():\n",
    "            if transformer is not None:\n",
    "                # Transform data\n",
    "                X_transformed = transformer.transform(X_imputed)\n",
    "                feature_names = transformer.get_feature_names_out(input_features=numerical.columns)\n",
    "                feature_names = [f'{name}_{feat}' for feat in feature_names]\n",
    "            else:\n",
    "                # Raw data (no transformation)\n",
    "                X_transformed = X_imputed\n",
    "                feature_names = [f'raw_{feat}' for feat in numerical.columns]\n",
    "\n",
    "            # Create DataFrame\n",
    "            X_df = pd.DataFrame(X_transformed, columns=feature_names)\n",
    "\n",
    "            # Store transformed features\n",
    "            transformed_features.append(X_df)\n",
    "            transformed_feature_names.append(name)\n",
    "\n",
    "        # Handle categorical features\n",
    "        if categorical is not None:\n",
    "            X_cat = categorical.reset_index(drop=True)\n",
    "        else:\n",
    "            X_cat = None\n",
    "\n",
    "        # Initialize list to collect base estimator predictions\n",
    "        base_preds = []\n",
    "\n",
    "        # If base_estimators are specified and not empty\n",
    "        if self.base_estimators_types and self.base_estimators_types != ['']:\n",
    "            for idx, name in enumerate(transformed_feature_names):\n",
    "                X_df = transformed_features[idx]\n",
    "\n",
    "                # Append categorical features if any\n",
    "                if X_cat is not None:\n",
    "                    X_df = pd.concat([X_df.reset_index(drop=True), X_cat], axis=1)\n",
    "                    # Update cat_features indices to point to the categorical features appended at the end\n",
    "                    new_cat_features = list(range(X_df.shape[1] - len(self.cat_features), X_df.shape[1]))\n",
    "                else:\n",
    "                    new_cat_features = []\n",
    "\n",
    "                for est_type in self.base_estimators_types:\n",
    "                    if est_type == 'raw':\n",
    "                        # Include transformed features directly as meta-features\n",
    "                        base_preds.append(X_df.copy())\n",
    "                        continue  # Skip any further processing\n",
    "                    est_name = f'{name}_{est_type}'\n",
    "                    if est_name not in self.base_estimators:\n",
    "                        raise ValueError(f\"Base estimator '{est_name}' has not been trained.\")\n",
    "                    estimator = self.base_estimators[est_name]\n",
    "                    pred = estimator.predict(X_df)\n",
    "\n",
    "                    # Ensure predictions are 2D arrays\n",
    "                    if pred.ndim == 1:\n",
    "                        pred = pred.reshape(-1, 1)\n",
    "\n",
    "                    # Generate appropriate column names for multi-output\n",
    "                    n_outputs = pred.shape[1] if pred.ndim > 1 else 1\n",
    "                    if n_outputs == 1:\n",
    "                        columns = [est_name]\n",
    "                    else:\n",
    "                        columns = [f'{est_name}_output_{i}' for i in range(n_outputs)]\n",
    "\n",
    "                    # Create DataFrame with correct column names\n",
    "                    base_preds.append(pd.DataFrame(pred, columns=columns))\n",
    "\n",
    "            # Combine base estimator predictions\n",
    "            if base_preds:\n",
    "                X_meta = pd.concat(base_preds, axis=1).reset_index(drop=True)\n",
    "            else:\n",
    "                raise ValueError(\"Base estimators did not produce any predictions.\")\n",
    "        else:\n",
    "            # If no base estimators, use transformed features directly\n",
    "            X_meta = pd.concat(transformed_features, axis=1).reset_index(drop=True)\n",
    "\n",
    "            # Append categorical features if any\n",
    "            if X_cat is not None:\n",
    "                X_meta = pd.concat([X_meta.reset_index(drop=True), X_cat], axis=1)\n",
    "                # Update cat_features indices to point to the categorical features appended at the end\n",
    "                new_cat_features = list(range(X_meta.shape[1] - len(self.cat_features), X_meta.shape[1]))\n",
    "            else:\n",
    "                new_cat_features = []\n",
    "\n",
    "        # Get final predictions from final estimators\n",
    "        final_preds = []\n",
    "        for est_type in self.final_estimators_types:\n",
    "            est_name = f'final_{est_type}'\n",
    "            if est_type not in self.final_estimators:\n",
    "                raise ValueError(f\"Final estimator '{est_type}' has not been trained.\")\n",
    "            final_estimator = self.final_estimators[est_type]\n",
    "            pred = final_estimator.predict(X_meta)\n",
    "\n",
    "            if pred.ndim != 1:\n",
    "                pred = pred[:, TARGET_COLS.index(\"sii\")]\n",
    "\n",
    "            final_preds.append(pred)\n",
    "\n",
    "        # Average predictions if multiple final estimators\n",
    "        if len(final_preds) == 1:\n",
    "            return final_preds[0]\n",
    "        else:\n",
    "            return np.mean(final_preds, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "papermill": {
     "duration": 66.117441,
     "end_time": "2024-09-20T23:29:00.772381",
     "exception": false,
     "start_time": "2024-09-20T23:27:54.654940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reset cached models\n",
    "models: list = []\n",
    "y_pred = np.full((X.height,), fill_value=np.nan)\n",
    "\n",
    "# Cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=52)\n",
    "rskf = RepeatedStratifiedKFold(n_repeats=3, n_splits=5, random_state=52)\n",
    "\n",
    "train_S = []\n",
    "test_S = []\n",
    "\n",
    "for train_idx, val_idx in skf.split(X, y_sii):\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Define your base estimator parameters\n",
    "    base_xgboost_params = dict(\n",
    "        objective=\"reg:squarederror\",\n",
    "        eval_metric=\"rmse\",\n",
    "        n_estimators=10000,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=5,\n",
    "        early_stopping_rounds=50,\n",
    "        tree_method='hist'\n",
    "    )\n",
    "    \n",
    "    base_catboost_params = dict(\n",
    "        loss_function=\"MultiRMSE\",\n",
    "        eval_metric=\"MultiRMSE\",\n",
    "        n_estimators=8000,  # Adjust as needed\n",
    "        learning_rate=0.05,\n",
    "        depth=5,\n",
    "        early_stopping_rounds=50,\n",
    "        bagging_temperature=15,\n",
    "        l2_leaf_reg=5\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Define your final parameters\n",
    "    spline_xgboost_params = dict(\n",
    "        objective=\"reg:squarederror\",\n",
    "        eval_metric=\"rmse\",\n",
    "        n_estimators=10000,\n",
    "        learning_rate=0.02,\n",
    "        max_depth=6,\n",
    "        early_stopping_rounds=50,\n",
    "        tree_method='hist'\n",
    "    )\n",
    "    \n",
    "    spline_catboost_params = dict(\n",
    "        loss_function=\"MultiRMSE\",\n",
    "        eval_metric=\"MultiRMSE\",\n",
    "        n_estimators=8000,  # Adjust as needed\n",
    "        learning_rate=0.05,\n",
    "        depth=5,\n",
    "        early_stopping_rounds=50,\n",
    "        bagging_temperature=15,\n",
    "        l2_leaf_reg=5\n",
    "    )\n",
    "    \n",
    "    lgbm = {'learning_rate': 0.013138686629412939, 'max_depth': 3, 'num_leaves': 97, 'min_data_in_leaf': 23,\n",
    "                 'feature_fraction': 0.9639520991011302, 'bagging_fraction': 0.6891406842317377, 'bagging_freq': 4,\n",
    "                 'lambda_l1': 9.291634503986419, 'lambda_l2': 1.6972510183080134, 'n_estimators': 174}\n",
    "    l = lgb.LGBMRegressor(**lgbm)\n",
    "        \n",
    "        \n",
    "    # Initialize and train model\n",
    "    SMC_params = dict(\n",
    "        base_catboost_params=base_catboost_params,\n",
    "        base_xgboost_params=base_xgboost_params,\n",
    "        spline_catboost_params=spline_catboost_params,\n",
    "        spline_xgboost_params=spline_xgboost_params,\n",
    "        spline_lgbm_params=lgbm,\n",
    "        use_spline=True,\n",
    "        use_poly=True,\n",
    "        use_raw=True,\n",
    "        base_estimators=['catboost', 'xgboost'],\n",
    "        final_estimators=['catboost', 'lgbm']\n",
    "    )\n",
    "    model = SillyManCombinational(**SMC_params)\n",
    "    model.fit(\n",
    "        X_train.to_pandas(),\n",
    "        y_train.to_pandas(),\n",
    "        eval_set=(X_val.to_pandas(), y_val.to_pandas()),\n",
    "        cat_features=cat_features,\n",
    "        verbose=False,\n",
    "    )\n",
    "    models.append(model)\n",
    "    \n",
    "    # Predict\n",
    "    y_train_pred = model.predict(X_train.to_pandas())\n",
    "    y_val_pred = model.predict(X_val.to_pandas())# / 2 \n",
    "    y_pred[val_idx] = y_val_pred\n",
    "    \n",
    "    train_kappa = quadratic_weighted_kappa(y_train['sii'], y_train_pred.round(0).astype(int))\n",
    "    val_kappa = quadratic_weighted_kappa(y_val['sii'], y_val_pred.round(0).astype(int))\n",
    "\n",
    "    train_S.append(train_kappa)\n",
    "    test_S.append(val_kappa)\n",
    "    \n",
    "\n",
    "assert np.isnan(y_pred).sum() == 0\n",
    "\n",
    "# Optimize thresholds\n",
    "optimizer = OptimizedRounder(n_classes=4, n_trials=400)\n",
    "optimizer.fit(y_pred, y_sii)\n",
    "y_pred_rounded = optimizer.predict(y_pred)\n",
    "\n",
    "# Calculate QWK\n",
    "qwk = cohen_kappa_score(y_sii, y_pred_rounded, weights=\"quadratic\")\n",
    "print(f\"Cross-Validated QWK Score: {qwk}\")\n",
    "\n",
    "print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                          x0=[0.5, 1.5, 2.5], args=(y['sii'], y_pred), \n",
    "                          method='Nelder-Mead') # Nelder-Mead | # Powell\n",
    "assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "\n",
    "oof_tuned = threshold_Rounder(y_pred, KappaOPtimizer.x)\n",
    "tKappa = quadratic_weighted_kappa(y['sii'], oof_tuned)\n",
    "\n",
    "PrintColor(f\"----> || Optimized QWK SCORE :: {tKappa:.3f}\", Fore.CYAN, Style.RESET_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Baseline:\n",
    "\n",
    "    Cross-Validated QWK Score: 0.455817591057746\n",
    "    Mean Train QWK --> 0.7968\n",
    "    Mean Validation QWK ---> 0.4070\n",
    "    ----> || Optimized QWK SCORE :: 0.446"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Cached\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "51F LG + CAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "CAT ALONE\n",
    "\n",
    "    Cross-Validated QWK Score: 0.4708138519619697\n",
    "    Mean Train QWK --> 0.4543\n",
    "    Mean Valdation QWK ---> 0.3812\n",
    "    ----> || Optimized QWK SCORE :: 0.470"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "LG + CAT ALONE\n",
    "\n",
    "    Cross-Validated QWK Score: 0.4722526498188261\n",
    "    Mean Train QWK --> 0.4237\n",
    "    Mean Validation QWK ---> 0.3763\n",
    "    ----> || Optimized QWK SCORE :: 0.377\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "LG ALONE\n",
    "\n",
    "    Cross-Validated QWK Score: 0.455740190965767\n",
    "    Mean Train QWK --> 0.3984\n",
    "    Mean Validation QWK ---> 0.3705\n",
    "    ----> || Optimized QWK SCORE :: 0.375"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "CAT\n",
    "\n",
    "    Cross-Validated QWK Score: 0.45019614229742055\n",
    "    Mean Train QWK --> 0.5248\n",
    "    Mean Validation QWK ---> 0.3727\n",
    "    ----> || Optimized QWK SCORE :: 0.445"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "LG + CAT\n",
    "\n",
    "    Cross-Validated QWK Score: 0.4334766068391631\n",
    "    Mean Train QWK --> 0.6582\n",
    "    Mean Validation QWK ---> 0.3809\n",
    "    ----> || Optimized QWK SCORE :: 0.433"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "b4 hp tune\n",
    "    Cross-Validated QWK Score: 0.3893511427225531\n",
    "    Mean Train QWK --> 0.8859\n",
    "    Mean Validation QWK ---> 0.3782\n",
    "    ----> || Optimized QWK SCORE :: 0.391\n",
    "after hp tune\n",
    "    Cross-Validated QWK Score: 0.41985059741531017\n",
    "    Mean Train QWK --> 0.7487\n",
    "    Mean Validation QWK ---> 0.3906\n",
    "    ----> || Optimized QWK SCORE :: 0.422"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# raw+cb fitting lgbm\n",
    "Mean Train QWK --> 0.8489\n",
    "Mean Validation QWK ---> 0.3776\n",
    "----> || Optimized QWK SCORE :: 0.392"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### triple modal overfit\n",
    "Mean Train QWK --> 0.7067\n",
    "Mean Validation QWK ---> 0.3896\n",
    "----> || Optimized QWK SCORE :: 0.425"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Cat base, but xgboost\n",
    "Mean Train QWK --> 0.7673\n",
    "Mean Validation QWK ---> 0.3837\n",
    "----> || Optimized QWK SCORE :: 0.396"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "#### My features:\n",
    "Mean Train QWK --> 0.7129\n",
    "Mean Validation QWK ---> 0.3701\n",
    "----> || Optimized QWK SCORE ::  0.433"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## LGBM\n",
    "#### My features:\n",
    "Mean Train QWK --> 0.7233\n",
    "Mean Validation QWK ---> 0.3604\n",
    "----> || Optimized QWK SCORE ::  0.420"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Catboost\n",
    "#### My features:\n",
    "Mean Train QWK --> 0.4543\n",
    "Mean Validation QWK ---> 0.3812\n",
    "----> || Optimized QWK SCORE ::  0.470"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "#### Sheikh's Features\n",
    "Mean Train QWK --> 0.5592\n",
    "Mean Validation QWK ---> 0.3788\n",
    "----> || Optimized QWK SCORE ::  0.458"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize thresholds\n",
    "optimizer = OptimizedRounder(n_classes=4, n_trials=400)\n",
    "optimizer.fit(y_pred, y_sii)\n",
    "y_pred_rounded = optimizer.predict(y_pred)\n",
    "\n",
    "# Calculate QWK\n",
    "qwk = cohen_kappa_score(y_sii, y_pred_rounded, weights=\"quadratic\")\n",
    "print(f\"Cross-Validated QWK Score: {qwk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {
    "papermill": {
     "duration": 0.009041,
     "end_time": "2024-09-20T23:29:00.790270",
     "exception": false,
     "start_time": "2024-09-20T23:29:00.781229",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# RESULTS!\n",
    "\n",
    "SMP Spline  - Cross-Validated QWK Score: 0.47250120300098075\n",
    "SMP Poly    - Cross-Validated QWK Score: 0.4590211420198351\n",
    "SMP SP      - Cross-Validated QWK Score: 0.45544920069658346\n",
    "SMP RSP CAT - Cross-Validated QWK Score: 0.473715731738207\n",
    "SMP RSP XG  - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "papermill": {
     "duration": 0.021183,
     "end_time": "2024-09-20T23:29:00.854965",
     "exception": false,
     "start_time": "2024-09-20T23:29:00.833782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AvgModel:\n",
    "    def __init__(self, models: list[BaseEstimator]):\n",
    "        self.models = models\n",
    "\n",
    "    def predict(self, X: ArrayLike) -> NDArray[np.int_]:\n",
    "        preds: list[NDArray[np.int_]] = []\n",
    "        for model in self.models:\n",
    "            pred = model.predict(X)\n",
    "            preds.append(pred)\n",
    "\n",
    "        return np.mean(preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "papermill": {
     "duration": 0.051348,
     "end_time": "2024-09-20T23:29:00.914906",
     "exception": false,
     "start_time": "2024-09-20T23:29:00.863558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optuna Submission\n",
    "'''avg_model = AvgModel(models)\n",
    "test_pred = avg_model.predict(X_test.to_pandas())\n",
    "test_pred_rounded = optimizer.predict(test_pred)\n",
    "test.select(\"id\").with_columns(\n",
    "    pl.Series(\"sii\", pl.Series(\"sii\", test_pred_rounded)),\n",
    ").write_csv(\"submission.csv\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scipy Submission\n",
    "avg_model = AvgModel(models)\n",
    "test_pred = avg_model.predict(X_test.to_pandas())\n",
    "test_pred_rounded = threshold_Rounder(test_pred, KappaOPtimizer.x)\n",
    "test.select(\"id\").with_columns(\n",
    "    pl.Series(\"sii\", pl.Series(\"sii\", test_pred_rounded)),\n",
    ").write_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "Experimental CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import optuna\n",
    "from sklearn.preprocessing import (\n",
    "    PolynomialFeatures,\n",
    "    SplineTransformer,\n",
    "    StandardScaler,\n",
    "    MinMaxScaler,\n",
    "    QuantileTransformer\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Define the SillyManCombinational class here (include the entire class code from your provided model)\n",
    "# For brevity, assume the class is already defined or imported\n",
    "\n",
    "# Load your data into X and y\n",
    "# For example:\n",
    "# X = pd.read_csv('X.csv')\n",
    "# y = pd.read_csv('y.csv')\n",
    "\n",
    "\n",
    "# For the purpose of this script, let's assume X and y are already loaded and are pandas DataFrames.\n",
    "\n",
    "# Also define y_sii\n",
    "y_sii = y['sii']\n",
    "\n",
    "# Define categorical features\n",
    "cat_features = []  # Replace with your list of categorical feature names if any\n",
    "\n",
    "# Define your base estimator parameters\n",
    "base_xgboost_params = dict(\n",
    "    objective=\"reg:squarederror\",\n",
    "    eval_metric=\"rmse\",\n",
    "    n_estimators=10000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    early_stopping_rounds=50,\n",
    "    tree_method='hist'\n",
    ")\n",
    "\n",
    "base_catboost_params = dict(\n",
    "    loss_function=\"MultiRMSE\",\n",
    "    eval_metric=\"MultiRMSE\",\n",
    "    n_estimators=8000,  # Adjust as needed\n",
    "    learning_rate=0.05,\n",
    "    depth=5,\n",
    "    early_stopping_rounds=50,\n",
    "    bagging_temperature=15,\n",
    "    l2_leaf_reg=5\n",
    ")\n",
    "\n",
    "# Define your final parameters\n",
    "spline_xgboost_params = dict(\n",
    "    objective=\"reg:squarederror\",\n",
    "    eval_metric=\"rmse\",\n",
    "    n_estimators=10000,\n",
    "    learning_rate=0.02,\n",
    "    max_depth=6,\n",
    "    early_stopping_rounds=50,\n",
    "    tree_method='hist'\n",
    ")\n",
    "\n",
    "spline_catboost_params = dict(\n",
    "    loss_function=\"MultiRMSE\",\n",
    "    eval_metric=\"MultiRMSE\",\n",
    "    n_estimators=8000,  # Adjust as needed\n",
    "    learning_rate=0.05,\n",
    "    depth=5,\n",
    "    early_stopping_rounds=50,\n",
    "    bagging_temperature=15,\n",
    "    l2_leaf_reg=5\n",
    ")\n",
    "\n",
    "def objective(trial):\n",
    "    # Sample hyperparameters\n",
    "    lgbm_params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 5, 50),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 10.0),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 10.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 20, 300),\n",
    "        'random_state': 47,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    # Prepare cross-validation\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=52)\n",
    "    \n",
    "    # Arrays to store the QWK scores\n",
    "    val_qwk_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(X, y_sii):\n",
    "        X_train, X_val = X.to_pandas().iloc[train_idx], X.to_pandas().iloc[val_idx]\n",
    "        y_train, y_val = y.to_pandas().iloc[train_idx], y.to_pandas().iloc[val_idx]\n",
    "        \n",
    "        # Update the model parameters\n",
    "        SMC_params = dict(\n",
    "            base_catboost_params=base_catboost_params,\n",
    "            base_xgboost_params=base_xgboost_params,\n",
    "            spline_catboost_params=spline_catboost_params,\n",
    "            spline_xgboost_params=spline_xgboost_params,\n",
    "            spline_lgbm_params=lgbm_params,  # Use the sampled params\n",
    "            use_spline=True,\n",
    "            use_poly=True,\n",
    "            use_raw=True,\n",
    "            base_estimators=['raw', 'catboost', 'xgboost'],\n",
    "            final_estimators=['lgbm']\n",
    "        )\n",
    "        \n",
    "        # Instantiate and train the model\n",
    "        model = SillyManCombinational(**SMC_params)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=(X_val, y_val),\n",
    "            cat_features=cat_features,\n",
    "            verbose=False,\n",
    "        )\n",
    "        \n",
    "        # Predict on validation set\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        \n",
    "        # Compute QWK\n",
    "        val_kappa = cohen_kappa_score(y_val['sii'], y_val_pred.round(0).astype(int), weights='quadratic')\n",
    "        val_qwk_scores.append(val_kappa)\n",
    "    \n",
    "    # Return the negative mean QWK score\n",
    "    return -np.mean(val_qwk_scores)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best parameters:\", best_params)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "Old params:     lgbm = {'learning_rate': 0.04603534510792164, 'max_depth': 9, 'num_leaves': 320, 'min_data_in_leaf': 13,\n",
    "          'feature_fraction': 0.8935304204489449, 'bagging_fraction': 0.7840117449237969, 'bagging_freq': 4,\n",
    "          'lambda_l1': 6.596560434072009, 'lambda_l2': 2} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Best parameters: {'learning_rate': 0.013138686629412939, 'max_depth': 3, 'num_leaves': 97, 'min_data_in_leaf': 23, 'feature_fraction': 0.9639520991011302, 'bagging_fraction': 0.6891406842317377, 'bagging_freq': 4, 'lambda_l1': 9.291634503986419, 'lambda_l2': 1.6972510183080134, 'n_estimators': 174}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9643020,
     "sourceId": 81933,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 75.110848,
   "end_time": "2024-09-20T23:29:01.764645",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-20T23:27:46.653797",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
