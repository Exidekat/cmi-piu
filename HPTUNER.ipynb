{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "papermill": {
     "duration": 3.303708,
     "end_time": "2024-09-20T23:27:53.232262",
     "exception": false,
     "start_time": "2024-09-20T23:27:49.928554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import re\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "\n",
    "from catboost import CatBoostRegressor, MultiTargetCustomMetric\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from numpy.typing import ArrayLike, NDArray\n",
    "from scipy.optimize import minimize\n",
    "from polars.testing import assert_frame_equal\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from colorama import Fore, Style, init\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Color printing\n",
    "def PrintColor(text: str, color = Fore.BLUE, style = Style.BRIGHT):\n",
    "    \"Prints color outputs using colorama using a text F-string\"\n",
    "    print(style + color + text + Style.RESET_ALL)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"Failed to optimize method\")\n",
    "\n",
    "\n",
    "#DATA_DIR = Path(\"/kaggle/input/child-mind-institute-problematic-internet-use\")\n",
    "DATA_DIR = Path(\"./data\")\n",
    "\n",
    "TARGET_COLS = [\n",
    "    \"PCIAT-PCIAT_01\",\n",
    "    \"PCIAT-PCIAT_02\",\n",
    "    \"PCIAT-PCIAT_03\",\n",
    "    \"PCIAT-PCIAT_04\",\n",
    "    \"PCIAT-PCIAT_05\",\n",
    "    \"PCIAT-PCIAT_06\",\n",
    "    \"PCIAT-PCIAT_07\",\n",
    "    \"PCIAT-PCIAT_08\",\n",
    "    \"PCIAT-PCIAT_09\",\n",
    "    \"PCIAT-PCIAT_10\",\n",
    "    \"PCIAT-PCIAT_11\",\n",
    "    \"PCIAT-PCIAT_12\",\n",
    "    \"PCIAT-PCIAT_13\",\n",
    "    \"PCIAT-PCIAT_14\",\n",
    "    \"PCIAT-PCIAT_15\",\n",
    "    \"PCIAT-PCIAT_16\",\n",
    "    \"PCIAT-PCIAT_17\",\n",
    "    \"PCIAT-PCIAT_18\",\n",
    "    \"PCIAT-PCIAT_19\",\n",
    "    \"PCIAT-PCIAT_20\",\n",
    "    \"PCIAT-PCIAT_Total\",\n",
    "    \"sii\",\n",
    "]\n",
    "\n",
    "FEATURE_COLS = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex', 'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI', 'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference', 'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP', 'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage', 'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec', 'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND', 'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU', 'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR', 'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season', 'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI', 'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM', 'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num', 'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM', 'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season', 'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw', 'SDS-SDS_Total_T', 'PreInt_EduHx-Season', 'PreInt_EduHx-computerinternet_hoursday', 'X_std', 'X_min', 'X_5', 'Y_std', 'Y_45', 'Z_min', 'anglez_std', 'anglez_min', 'anglez_max', 'light_max', 'battery_voltage_mean', 'battery_voltage_std']\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    \"Basic_Demos-Age\",\n",
    "    \"PreInt_EduHx-computerinternet_hoursday\",\n",
    "    \"SDS-SDS_Total_Raw\",\n",
    "    \"Physical-Height\",\n",
    "    \"FGC-FGC_TL\",\n",
    "    \"Physical-Waist_Circumference\",\n",
    "    \"BIA-BIA_FMI\",\n",
    "    \"BIA-BIA_Fat\",\n",
    "    \"Fitness_Endurance-Time_Sec\",\n",
    "    \"Fitness_Endurance-Max_Stage\",\n",
    "    \"FGC-FGC_CU_Zone\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit to Ravi Ramakrishnan for his great Preprocessing class\n",
    "# https://www.kaggle.com/code/ravi20076/cmi2024-baseline-v2\n",
    "\n",
    "class Preprocessor:\n",
    "    \"This class organizes the preprocessing steps for the train-test data into a single code block\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, cat_imp_val : str= \"missing\", ip_path: str = DATA_DIR\n",
    "    ):\n",
    "        self.cat_imp_val = cat_imp_val\n",
    "        self.ip_path     = ip_path\n",
    "        \n",
    "    def make_pqfile_cols(\n",
    "        self, verbose: bool = False, label: str = \"Train\"\n",
    "    )->pl.DataFrame:\n",
    "        \"This method collates the id level parquet files and creates the aggregation columns in a polars dataframe\"\n",
    "\n",
    "        cols = [\"X\", \"Y\", \"Z\", \"enmo\", \"anglez\", \"light\", \"battery_voltage\"]\n",
    "        \n",
    "        ip_path   = os.path.join(self.ip_path, f\"series_{label.lower()}.parquet\")\n",
    "        all_files = os.listdir(ip_path)\n",
    "\n",
    "        for file_nb, file in tqdm(enumerate(all_files)):\n",
    "            df = \\\n",
    "            pl.scan_parquet(\n",
    "                os.path.join(ip_path, file, f\"part-0.parquet\")\n",
    "            ).select(pl.col(cols)).\\\n",
    "            collect().\\\n",
    "            describe(\n",
    "                percentiles = np.arange(0.05, 0.95, 0.10)\n",
    "            ).\\\n",
    "            filter(~pl.col(\"statistic\").is_in([\"count\", \"null_count\"])).\\\n",
    "            unpivot(index = \"statistic\").\\\n",
    "            with_columns(\n",
    "                pl.concat_str([pl.col(\"variable\"), pl.col(\"statistic\")],separator = \"_\",).alias(\"myvar\")\n",
    "            ).\\\n",
    "            with_columns(pl.col(\"myvar\").str.replace(r\"\\%\", \"\")).\\\n",
    "            select([\"myvar\", \"value\"]).\\\n",
    "            transpose(column_names = \"myvar\").\\\n",
    "            select(pl.all().shrink_dtype()).\\\n",
    "            with_columns(\n",
    "                pl.Series(\"id\", np.array(re.sub(\"id=\", \"\", file)))\n",
    "            )\n",
    "\n",
    "            if file_nb == 0:\n",
    "                op_df = df.clone()\n",
    "            elif file_nb > 0:\n",
    "                op_df = pl.concat([op_df, df], how = \"vertical_relaxed\")\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"---> Shapes = {op_df.shape}\")\n",
    "                else:\n",
    "                    pass\n",
    "            del df\n",
    "\n",
    "        PrintColor(f\"---> {label} - shape = {op_df.shape}\", color = Fore.CYAN)\n",
    "        return op_df\n",
    "\n",
    "    def pp_data(\n",
    "        self, df: pl.DataFrame, label: str = \"Train\", cat_cols: list = [], \n",
    "    ):\n",
    "        \"This method preprocesses the train-test data with requisite steps\"\n",
    "        \n",
    "        PrintColor(f\"\\n --- Data Processing - {label} --- \\n\")\n",
    "        PrintColor(f\"---> Shape = {df.shape} - memory usage {df.estimated_size('mb') :.3f} MB\", \n",
    "                   color = Fore.CYAN\n",
    "                  )\n",
    "        \n",
    "        if label == \"Train\":\n",
    "            cat_cols = df.select(cs.string().exclude(\"id\")).columns\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        df    = df.with_columns(pl.col(cat_cols).fill_null(self.cat_imp_val).cast(pl.Categorical))\n",
    "        op_df = self.make_pqfile_cols(label = label)\n",
    "        df    = df.join(op_df, how = \"left\", on = \"id\")\n",
    "        df    = df.select(pl.all().shrink_dtype())\n",
    "        del op_df\n",
    "        \n",
    "        PrintColor(f\"---> Shape = {df.shape} - memory usage {df.estimated_size('mb') :.3f} MB\", \n",
    "                   color = Fore.CYAN\n",
    "                  )\n",
    "        return df, cat_cols\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "papermill": {
     "duration": 0.202543,
     "end_time": "2024-09-20T23:27:53.443436",
     "exception": false,
     "start_time": "2024-09-20T23:27:53.240893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pl.read_csv(DATA_DIR / \"train.csv\").drop(['PCIAT-Season',], strict=False)#.drop(['Basic_Demos-Enroll_Season', 'CGAS-Season'], strict=False)\n",
    "test = pl.read_csv(DATA_DIR / \"test.csv\").drop(['PCIAT-Season'], strict=False)#.drop(['Basic_Demos-Enroll_Season', 'CGAS-Season'], strict=False)\n",
    "\n",
    "# Ensure 'id' is a string in both DataFrames\n",
    "train = train.with_columns(pl.col('id').cast(pl.Utf8))\n",
    "test = test.with_columns(pl.col('id').cast(pl.Utf8))\n",
    "\n",
    "pp = Preprocessor()\n",
    "\n",
    "# Use StringCache to handle categoricals consistently\n",
    "with pl.StringCache():\n",
    "    train, cat_cols = pp.pp_data(train, \"Train\")\n",
    "    test, _ = pp.pp_data(test, \"Test\", cat_cols)\n",
    "\n",
    "# Ensure data types match between train and test\n",
    "for col in train.columns:\n",
    "    if col in test.columns:\n",
    "        train_dtype = train.schema[col]\n",
    "        test_dtype = test.schema[col]\n",
    "        if train_dtype != test_dtype:\n",
    "            # Cast test column to train's data type\n",
    "            test = test.with_columns(pl.col(col).cast(train_dtype))\n",
    "    else:\n",
    "        # Column in train but not in test, fill with nulls in test\n",
    "        test = test.with_columns(pl.lit(None).cast(train.schema[col]).alias(col))\n",
    "\n",
    "# Reorder test columns to match train\n",
    "test = test.select(train.columns)\n",
    "\n",
    "# Concatenate train and test\n",
    "train_test = pl.concat([train, test], how=\"vertical\")\n",
    "\n",
    "IS_TEST = test.height <= 100\n",
    "\n",
    "assert_frame_equal(train, train_test[: train.height].select(train.columns))\n",
    "assert_frame_equal(test, train_test[train.height :].select(test.columns))\n",
    "\n",
    "IS_TEST = test.height <= 100\n",
    "\n",
    "assert_frame_equal(train, train_test[: train.height].select(train.columns))\n",
    "assert_frame_equal(test, train_test[train.height :].select(test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "papermill": {
     "duration": 0.066498,
     "end_time": "2024-09-20T23:27:53.518169",
     "exception": false,
     "start_time": "2024-09-20T23:27:53.451671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Concatenate train and test\n",
    "train_test = pl.concat([train, test], how=\"vertical\")\n",
    "\n",
    "# Cast string columns to categorical\n",
    "train_test = train_test.with_columns(cs.string().cast(pl.Categorical).fill_null(\"NAN\"))\n",
    "\n",
    "# Get cat_features and num_features\n",
    "cat_features = train_test.select(FEATURE_COLS).select(cs.categorical()).columns\n",
    "print(\"Cat Features selected:\")\n",
    "print(cat_features)  # Should be none\n",
    "\n",
    "num_features = train_test.select(FEATURE_COLS).select(cs.numeric()).columns\n",
    "print(\"Numerical Features selected:\")\n",
    "print(num_features)\n",
    "\n",
    "# Impute missing values in numerical features\n",
    "imputation_strategy='mean'  # imputation_strategy (str): Strategy for imputing missing values ('mean', 'median', 'most_frequent', etc.).\n",
    "imputer = SimpleImputer(strategy=imputation_strategy)\n",
    "train_test[num_features] = imputer.fit_transform(train_test[num_features])\n",
    "\n",
    "train = train_test[: train.height]\n",
    "test = train_test[train.height :]\n",
    "\n",
    "# ignore rows with null values in TARGET_COLS\n",
    "train_without_null = train_test.drop_nulls(subset=TARGET_COLS)\n",
    "X = train_without_null.select(FEATURE_COLS)\n",
    "X_test = test.select(FEATURE_COLS)\n",
    "y = train_without_null.select(TARGET_COLS)\n",
    "y_sii = y.get_column(\"sii\").to_numpy()  # ground truth\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "papermill": {
     "duration": 0.008025,
     "end_time": "2024-09-20T23:27:53.534860",
     "exception": false,
     "start_time": "2024-09-20T23:27:53.526835",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tubotubo's Quadratic Weighted Kappa metric & Optuna optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "papermill": {
     "duration": 0.03706,
     "end_time": "2024-09-20T23:27:53.580139",
     "exception": false,
     "start_time": "2024-09-20T23:27:53.543079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiTargetQWK(MultiTargetCustomMetric):\n",
    "    def get_final_error(self, error, weight):\n",
    "        return np.sum(error)  # / np.sum(weight)\n",
    "\n",
    "    def is_max_optimal(self):\n",
    "        # if True, the bigger the better\n",
    "        return True\n",
    "\n",
    "    def evaluate(self, approxes, targets, weight):\n",
    "        # approxes: 予測値 (shape: [ターゲット数, サンプル数])\n",
    "        # targets: 実際の値 (shape: [ターゲット数, サンプル数])\n",
    "        # weight: サンプルごとの重み (Noneも可)\n",
    "\n",
    "        approx = np.clip(approxes[-1], 0, 3).round().astype(int)\n",
    "        target = targets[-1]\n",
    "\n",
    "        qwk = cohen_kappa_score(target, approx, weights=\"quadratic\")\n",
    "\n",
    "        return qwk, 1\n",
    "\n",
    "    def get_custom_metric_name(self):\n",
    "        return \"MultiTargetQWK\"\n",
    "\n",
    "\n",
    "class OptimizedRounder:\n",
    "    \"\"\"\n",
    "    A class for optimizing the rounding of continuous predictions into discrete class labels using Optuna.\n",
    "    The optimization process maximizes the Quadratic Weighted Kappa score by learning thresholds that separate\n",
    "    continuous predictions into class intervals.\n",
    "\n",
    "    Args:\n",
    "        n_classes (int): The number of discrete class labels.\n",
    "        n_trials (int, optional): The number of trials for the Optuna optimization. Defaults to 100.\n",
    "\n",
    "    Attributes:\n",
    "        n_classes (int): The number of discrete class labels.\n",
    "        labels (NDArray[np.int_]): An array of class labels from 0 to `n_classes - 1`.\n",
    "        n_trials (int): The number of optimization trials.\n",
    "        metric (Callable): The Quadratic Weighted Kappa score metric used for optimization.\n",
    "        thresholds (List[float]): The optimized thresholds learned after calling `fit()`.\n",
    "\n",
    "    Methods:\n",
    "        fit(y_pred: NDArray[np.float_], y_true: NDArray[np.int_]) -> None:\n",
    "            Fits the rounding thresholds based on continuous predictions and ground truth labels.\n",
    "\n",
    "            Args:\n",
    "                y_pred (NDArray[np.float_]): Continuous predictions that need to be rounded.\n",
    "                y_true (NDArray[np.int_]): Ground truth class labels.\n",
    "\n",
    "            Returns:\n",
    "                None\n",
    "\n",
    "        predict(y_pred: NDArray[np.float_]) -> NDArray[np.int_]:\n",
    "            Predicts discrete class labels by rounding continuous predictions using the fitted thresholds.\n",
    "            `fit()` must be called before `predict()`.\n",
    "\n",
    "            Args:\n",
    "                y_pred (NDArray[np.float_]): Continuous predictions to be rounded.\n",
    "\n",
    "            Returns:\n",
    "                NDArray[np.int_]: Predicted class labels.\n",
    "\n",
    "        _normalize(y: NDArray[np.float_]) -> NDArray[np.float_]:\n",
    "            Normalizes the continuous values to the range [0, `n_classes - 1`].\n",
    "\n",
    "            Args:\n",
    "                y (NDArray[np.float_]): Continuous values to be normalized.\n",
    "\n",
    "            Returns:\n",
    "                NDArray[np.float_]: Normalized values.\n",
    "\n",
    "    References:\n",
    "        - This implementation uses Optuna for threshold optimization.\n",
    "        - Quadratic Weighted Kappa is used as the evaluation metric.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_classes: int, n_trials: int = 100):\n",
    "        self.n_classes = n_classes\n",
    "        self.labels = np.arange(n_classes)\n",
    "        self.n_trials = n_trials\n",
    "        self.metric = partial(cohen_kappa_score, weights=\"quadratic\")\n",
    "\n",
    "    def fit(self, y_pred: NDArray[np.float_], y_true: NDArray[np.int_]) -> None:\n",
    "        y_pred = self._normalize(y_pred)\n",
    "\n",
    "        def objective(trial: optuna.Trial) -> float:\n",
    "            thresholds = []\n",
    "            for i in range(self.n_classes - 1):\n",
    "                low = max(thresholds) if i > 0 else min(self.labels)\n",
    "                high = max(self.labels)\n",
    "                th = trial.suggest_float(f\"threshold_{i}\", low, high)\n",
    "                thresholds.append(th)\n",
    "            try:\n",
    "                y_pred_rounded = np.digitize(y_pred, thresholds)\n",
    "            except ValueError:\n",
    "                return -100\n",
    "            return self.metric(y_true, y_pred_rounded)\n",
    "\n",
    "        optuna.logging.disable_default_handler()\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(\n",
    "            objective,\n",
    "            n_trials=self.n_trials,\n",
    "        )\n",
    "        self.thresholds = [study.best_params[f\"threshold_{i}\"] for i in range(self.n_classes - 1)]\n",
    "\n",
    "    def predict(self, y_pred: NDArray[np.float_]) -> NDArray[np.int_]:\n",
    "        assert hasattr(self, \"thresholds\"), \"fit() must be called before predict()\"\n",
    "        y_pred = self._normalize(y_pred)\n",
    "        return np.digitize(y_pred, self.thresholds)\n",
    "\n",
    "    def _normalize(self, y: NDArray[np.float_]) -> NDArray[np.float_]:\n",
    "        # normalize y_pred to [0, n_classes - 1]\n",
    "        return (y - y.min()) / (y.max() - y.min()) * (self.n_classes - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "papermill": {
     "duration": 0.008373,
     "end_time": "2024-09-20T23:27:53.597003",
     "exception": false,
     "start_time": "2024-09-20T23:27:53.588630",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Start making models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "papermill": {
     "duration": 0.040654,
     "end_time": "2024-09-20T23:27:54.646588",
     "exception": false,
     "start_time": "2024-09-20T23:27:54.605934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from catboost import CatBoostRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Updated SILLY MODEL with Polynomial Features and Imputation\n",
    "class SillyManPolynomial:\n",
    "    def __init__(self, cat_params, degree=2, include_bias=False):\n",
    "        \"\"\"\n",
    "        Initializes the SillyManPolynomial model.\n",
    "\n",
    "        Parameters:\n",
    "        - degree (int): Degree of the polynomial features.\n",
    "        - include_bias (bool): Whether to include a bias column.\n",
    "        \"\"\"\n",
    "        self.poly = PolynomialFeatures(degree=degree, include_bias=include_bias)\n",
    "        self.cat = CatBoostRegressor(**cat_params)\n",
    "        \n",
    "    def fit(self, x, y, eval_set, cat_features, verbose=False):\n",
    "        \"\"\"\n",
    "        Fits the model with polynomial feature generation and imputation.\n",
    "\n",
    "        Parameters:\n",
    "        - x (pd.DataFrame): Training features.\n",
    "        - y (pd.Series or pd.DataFrame): Training target.\n",
    "        - eval_set (tuple): Validation set as (X_val, y_val).\n",
    "        - cat_features (list): List of categorical feature names to exclude or handle separately.\n",
    "        - verbose (bool): Verbosity flag.\n",
    "        \"\"\"\n",
    "        self.cat_features = cat_features\n",
    "        \n",
    "        # Separate numerical features (assuming categorical features are to be excluded or handled separately)\n",
    "        if cat_features:\n",
    "            numerical = x.drop(columns=self.cat_features)\n",
    "            val_numerical = eval_set[0].drop(columns=self.cat_features)\n",
    "            categorical = x[self.cat_features]\n",
    "            val_categorical = eval_set[0][self.cat_features]\n",
    "        else:\n",
    "            numerical = x.copy()\n",
    "            val_numerical = eval_set[0]\n",
    "            categorical = None\n",
    "\n",
    "        # Apply Polynomial Feature Generation to numerical features\n",
    "        X_train_poly = np.concatenate((numerical, self.poly.fit_transform(numerical)), axis=1)\n",
    "        X_val_poly = np.concatenate((val_numerical, self.poly.transform(val_numerical)), axis=1)\n",
    "\n",
    "        # If there are categorical features, append them to the polynomial features\n",
    "        if categorical is not None:\n",
    "            X_train_cat = categorical.values\n",
    "            X_val_cat = val_categorical.values\n",
    "            X_train_processed = np.hstack([X_train_poly, X_train_cat])\n",
    "            X_val_processed = np.hstack([X_val_poly, X_val_cat])\n",
    "            # Update cat_features indices to point to the categorical features appended at the end\n",
    "            new_cat_features = list(range(X_train_poly.shape[1], X_train_processed.shape[1]))\n",
    "        else:\n",
    "            X_train_processed = X_train_poly\n",
    "            X_val_processed = X_val_poly\n",
    "            new_cat_features = []\n",
    "\n",
    "        # Fit CatBoost\n",
    "        self.cat.fit(\n",
    "            X_train_processed,\n",
    "            y,\n",
    "            eval_set=(X_val_processed, eval_set[1]),\n",
    "            cat_features=new_cat_features,  # Specify categorical features if any\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predicts using the fitted model.\n",
    "\n",
    "        Parameters:\n",
    "        - x (pd.DataFrame): Features for prediction.\n",
    "        - cat_features (list): List of categorical feature names.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: Predictions.\n",
    "        \"\"\"\n",
    "        # Separate numerical features\n",
    "        if cat_features:\n",
    "            numerical = x.drop(columns=self.cat_features)\n",
    "            categorical = x[self.cat_features]\n",
    "        else:\n",
    "            numerical = x.copy()\n",
    "            categorical = None\n",
    "        \n",
    "        # Apply Polynomial Feature Generation\n",
    "        X_poly = np.concatenate((numerical, self.poly.transform(numerical)), axis=1)\n",
    "        \n",
    "        # If there are categorical features, append them\n",
    "        if categorical is not None:\n",
    "            X_cat = categorical.values\n",
    "            X_processed = np.hstack([X_poly, X_cat])\n",
    "        else:\n",
    "            X_processed = X_poly\n",
    "\n",
    "        return self.cat.predict(X_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "papermill": {
     "duration": 66.117441,
     "end_time": "2024-09-20T23:29:00.772381",
     "exception": false,
     "start_time": "2024-09-20T23:27:54.654940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=52)\n",
    "models: list = []\n",
    "y_pred = np.full((X.height, len(TARGET_COLS)), fill_value=np.nan)\n",
    "\n",
    "train_S = []\n",
    "test_S = []\n",
    "\n",
    "for train_idx, val_idx in skf.split(X, y_sii):\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Define your parameters\n",
    "    cat_params = dict(\n",
    "        loss_function=\"MultiRMSE\",\n",
    "        eval_metric=\"MultiRMSE\",  # Ensure eval_metric is appropriate\n",
    "        iterations=8000,  # Adjust as needed\n",
    "        learning_rate=0.05,\n",
    "        depth=5,\n",
    "        early_stopping_rounds=50,\n",
    "    )\n",
    "    \n",
    "    # Initialize and train model\n",
    "    model = SillyManPolynomial(cat_params)\n",
    "    #model = CatBoostRegressor(**params)\n",
    "    model.fit(\n",
    "        X_train.to_pandas(),\n",
    "        y_train.to_pandas(),\n",
    "        eval_set=(X_val.to_pandas(), y_val.to_pandas()),\n",
    "        cat_features=cat_features,\n",
    "        verbose=False,\n",
    "    )\n",
    "    models.append(model)\n",
    "    \n",
    "    # Predict\n",
    "    y_train_pred = model.predict(X_train.to_pandas())\n",
    "    y_val_pred = model.predict(X_val.to_pandas())\n",
    "    y_pred[val_idx] = y_val_pred\n",
    "    \n",
    "    train_kappa = quadratic_weighted_kappa(y_train['sii'], y_train_pred[:, TARGET_COLS.index(\"sii\")].round(0).astype(int))\n",
    "    val_kappa = quadratic_weighted_kappa(y_val['sii'], y_val_pred[:, TARGET_COLS.index(\"sii\")].round(0).astype(int))\n",
    "\n",
    "    train_S.append(train_kappa)\n",
    "    test_S.append(val_kappa)\n",
    "\n",
    "assert np.isnan(y_pred).sum() == 0\n",
    "\n",
    "# Optimize thresholds\n",
    "optimizer = OptimizedRounder(n_classes=4, n_trials=300)\n",
    "y_pred_total = y_pred[:, TARGET_COLS.index(\"PCIAT-PCIAT_Total\")]\n",
    "optimizer.fit(y_pred_total, y_sii)\n",
    "y_pred_rounded = optimizer.predict(y_pred_total)\n",
    "\n",
    "# Calculate QWK\n",
    "qwk = cohen_kappa_score(y_sii, y_pred_rounded, weights=\"quadratic\")\n",
    "print(f\"Cross-Validated QWK Score: {qwk}\")\n",
    "\n",
    "print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                          x0=[0.5, 1.5, 2.5], args=(y['sii'], y_pred[:, TARGET_COLS.index(\"sii\")]), \n",
    "                          method='Nelder-Mead') # Nelder-Mead | # Powell\n",
    "assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "\n",
    "oof_tuned = threshold_Rounder(y_pred[:, TARGET_COLS.index(\"sii\")], KappaOPtimizer.x)\n",
    "tKappa = quadratic_weighted_kappa(y['sii'], oof_tuned)\n",
    "\n",
    "PrintColor(f\"----> || Optimized QWK SCORE :: {tKappa:.3f}\", Fore.CYAN, Style.RESET_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "papermill": {
     "duration": 0.021183,
     "end_time": "2024-09-20T23:29:00.854965",
     "exception": false,
     "start_time": "2024-09-20T23:29:00.833782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AvgModel:\n",
    "    def __init__(self, models: list[BaseEstimator]):\n",
    "        self.models = models\n",
    "\n",
    "    def predict(self, X: ArrayLike) -> NDArray[np.int_]:\n",
    "        preds: list[NDArray[np.int_]] = []\n",
    "        for model in self.models:\n",
    "            pred = model.predict(X)\n",
    "            preds.append(pred)\n",
    "\n",
    "        return np.mean(preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "papermill": {
     "duration": 0.051348,
     "end_time": "2024-09-20T23:29:00.914906",
     "exception": false,
     "start_time": "2024-09-20T23:29:00.863558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_model = AvgModel(models)\n",
    "test_pred = avg_model.predict(X_test.to_pandas())[:, TARGET_COLS.index(\"PCIAT-PCIAT_Total\")]\n",
    "test_pred_rounded = optimizer.predict(test_pred)\n",
    "test.select(\"id\").with_columns(\n",
    "    pl.Series(\"sii\", pl.Series(\"sii\", test_pred_rounded)),\n",
    ").write_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "papermill": {
     "duration": 0.00833,
     "end_time": "2024-09-20T23:29:00.931995",
     "exception": false,
     "start_time": "2024-09-20T23:29:00.923665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PrintColor(\"Success!\", Fore.GREEN, Style.RESET_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HP TUNE OR BUSSSST\n",
    "tr = 0\n",
    "\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    global tr\n",
    "    tr += 1\n",
    "    print(\"TRIAL\", tr)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=52)\n",
    "    models: list = []\n",
    "    y_pred = np.full((X.height, len(TARGET_COLS)), fill_value=np.nan)\n",
    "    \n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(X, y_sii):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Define your parameters\n",
    "        cat_params = dict(\n",
    "            loss_function=\"MultiRMSE\",\n",
    "            eval_metric=\"MultiRMSE\",\n",
    "            iterations=trial.suggest_int('iterations', 500, 12000),\n",
    "            learning_rate=trial.suggest_loguniform('learning_rate', 1e-4, 1e-1),\n",
    "            depth = trial.suggest_int('depth', 4, 10, step=1),\n",
    "            early_stopping_rounds = trial.suggest_int('early_stopping_rounds', 10, 1000),\n",
    "            bagging_temperature = trial.suggest_int('bagging_temperature', 0, 24, step=3),\n",
    "            l2_leaf_reg=trial.suggest_loguniform('l2_leaf_reg', 1e-3, 20.0),\n",
    "            random_strength=trial.suggest_int('random_strength', 1, 9, step=2),\n",
    "            colsample_bylevel=trial.suggest_loguniform('colsample_bylevel', 0.1, 1.0),\n",
    "            min_data_in_leaf=trial.suggest_loguniform('min_data_in_leaf', 1, 50),\n",
    "        )\n",
    "        \n",
    "        # Initialize and train model\n",
    "        model = SillyManPolynomial(cat_params)\n",
    "        #model = CatBoostRegressor(**params)\n",
    "        model.fit(\n",
    "            X_train.to_pandas(),\n",
    "            y_train.to_pandas(),\n",
    "            eval_set=(X_val.to_pandas(), y_val.to_pandas()),\n",
    "            cat_features=cat_features,\n",
    "            verbose=False,\n",
    "        )\n",
    "        models.append(model)\n",
    "        \n",
    "        # Predict\n",
    "        y_train_pred = model.predict(X_train.to_pandas())\n",
    "        y_val_pred = model.predict(X_val.to_pandas())\n",
    "        y_pred[val_idx] = y_val_pred\n",
    "        \n",
    "        train_kappa = quadratic_weighted_kappa(y_train['sii'], y_train_pred[:, TARGET_COLS.index(\"sii\")].round(0).astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val['sii'], y_val_pred[:, TARGET_COLS.index(\"sii\")].round(0).astype(int))\n",
    "    \n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "    \n",
    "    assert np.isnan(y_pred).sum() == 0\n",
    "    \n",
    "    # Optimize thresholds\n",
    "    optimizer = OptimizedRounder(n_classes=4, n_trials=300)\n",
    "    y_pred_total = y_pred[:, TARGET_COLS.index(\"PCIAT-PCIAT_Total\")]\n",
    "    optimizer.fit(y_pred_total, y_sii)\n",
    "    y_pred_rounded = optimizer.predict(y_pred_total)\n",
    "    \n",
    "    # Calculate QWK\n",
    "    qwk = cohen_kappa_score(y_sii, y_pred_rounded, weights=\"quadratic\")\n",
    "    print(f\"Cross-Validated QWK Score: {qwk}\")\n",
    "    \n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    mv = np.mean(test_S)\n",
    "    print(f\"Mean Validation QWK ---> {mv:.4f}\")\n",
    "    \n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(y['sii'], y_pred[:, TARGET_COLS.index(\"sii\")]), \n",
    "                              method='Nelder-Mead') # Nelder-Mead | # Powell\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    \n",
    "    oof_tuned = threshold_Rounder(y_pred[:, TARGET_COLS.index(\"sii\")], KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y['sii'], oof_tuned)\n",
    "    \n",
    "    PrintColor(f\"----> || Optimized QWK SCORE :: {tKappa:.3f}\", Fore.CYAN, Style.RESET_ALL)\n",
    "    return mv\n",
    "\n",
    "optuna.logging.enable_default_handler()\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=50,\n",
    "    n_jobs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Trial:')\n",
    "trial = study.best_trial\n",
    "print(f'  Value: {trial.value}')\n",
    "print('  Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9643020,
     "sourceId": 81933,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 75.110848,
   "end_time": "2024-09-20T23:29:01.764645",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-20T23:27:46.653797",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
